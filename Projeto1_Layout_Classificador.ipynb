{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Projeto 1 - Ci√™ncia dos Dados"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nome: Jo√£o Pedro Reis Lima\r\n",
    "\r\n",
    "Nome: Lucas Gurgel "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Aten√ß√£o: Ser√£o permitidos grupos de tr√™s pessoas, mas com uma rubrica mais exigente. Grupos deste tamanho precisar√£o fazer um question√°rio de avalia√ß√£o de trabalho em equipe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "Carregando algumas bibliotecas:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "source": [
    "%matplotlib inline\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "source": [
    "print('Esperamos trabalhar no diret√≥rio')\r\n",
    "print(os.getcwd())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Esperamos trabalhar no diret√≥rio\n",
      "c:\\Users\\Joao\\Projeto-1-CDADOS\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Carregando a base de dados com os tweets classificados como relevantes e n√£o relevantes:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "source": [
    "filename = 'esquadr√£o suicida 2.xlsx'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "source": [
    "dados = pd.read_excel(filename)\r\n",
    "dados.head(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>relevancia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to feliz q da p prestar aten√ß√£o e gostar de v√°...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>os roteirista de esquadr√£o suicida 2 devem ter...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>assistir esse esquadr√£o suicida 2 de marola</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e eu e gilson que fechamos a sala de cinema p ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>esquadr√£o suicida 2 √© muito bom, amei o filme</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Treinamento  relevancia\n",
       "0  to feliz q da p prestar aten√ß√£o e gostar de v√°...         1.0\n",
       "1  os roteirista de esquadr√£o suicida 2 devem ter...         0.0\n",
       "2        assistir esse esquadr√£o suicida 2 de marola         0.0\n",
       "3  e eu e gilson que fechamos a sala de cinema p ...         0.0\n",
       "4      esquadr√£o suicida 2 √© muito bom, amei o filme         1.0"
      ]
     },
     "metadata": {},
     "execution_count": 185
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "source": [
    "test = pd.read_excel(filename, sheet_name = 'Teste')\r\n",
    "test.head(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teste</th>\n",
       "      <th>relevancia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>assisti esquadr√£o suicida 2 com um pouco de at...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vou ver esquadr√£o suicida 2 hj, vcs j√° viram?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@lsdcomixxx adm hj eu sonhei que ele tava no e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ranking personagens do esquadr√£o suicida:\\n\\n1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mano gloria groove no esquadr√£o suicida 2 fico...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Teste  relevancia\n",
       "0  assisti esquadr√£o suicida 2 com um pouco de at...           0\n",
       "1      vou ver esquadr√£o suicida 2 hj, vcs j√° viram?           0\n",
       "2  @lsdcomixxx adm hj eu sonhei que ele tava no e...           0\n",
       "3  ranking personagens do esquadr√£o suicida:\\n\\n1...           1\n",
       "4  mano gloria groove no esquadr√£o suicida 2 fico...           1"
      ]
     },
     "metadata": {},
     "execution_count": 186
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "## Classificador autom√°tico de sentimento\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "N√≥s consideramos relavante coment√°rios que demostravam alguma critica ao produto tais como: Se a pessoa gostou ou n√£o da obra, o que ela sentiu(emo√ß√£o), tamb√©m consideramos personagens favoritos como algo relavante \r\n",
    "\r\n",
    "N√£o consideramos comentarios de situa√ß√µes adversas sem rela√ß√£o direta com a criticidade da obra, tal como comentarios pessoais"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "### Montando um Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "source": [
    "import re \r\n",
    "import emoji\r\n",
    "from emoji import UNICODE_EMOJI\r\n",
    "\r\n",
    "\r\n",
    "def cleanup(text):\r\n",
    "    \"\"\"\r\n",
    "        Fun√ß√£o de limpeza muito simples que troca alguns sinais b√°sicos por espa√ßos\r\n",
    "    \"\"\"\r\n",
    "    #import string\r\n",
    "    punctuation = '[‚Äù@\\-/!.:?;,''\"|()#$%¬®&*]' # Note que os sinais [] s√£o delimitadores de um conjunto.\r\n",
    "    pattern = re.compile(punctuation)\r\n",
    "    text_subbed = re.sub(pattern, '', text)\r\n",
    "    return text_subbed"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "source": [
    "def minusculo(text):\r\n",
    "    return text.lower()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "source": [
    "def separa_emoji(tweet):\r\n",
    "   \r\n",
    "    modified=' '.join(emoji.get_emoji_regexp().split(tweet))\r\n",
    "    modified=modified.split()\r\n",
    "    for i,emoji1 in enumerate(modified):\r\n",
    "        if emoji1 in UNICODE_EMOJI['pt']:\r\n",
    "            modified[i]=UNICODE_EMOJI['pt'][emoji1].replace(':','')\r\n",
    "        else:\r\n",
    "            continue\r\n",
    "    modified=' '.join(modified)\r\n",
    "        \r\n",
    "    return modified"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "source": [
    "def limpatudo(text):\r\n",
    "    tira_pontuacao = cleanup(text)\r\n",
    "    tudo_minusculo = minusculo(tira_pontuacao)\r\n",
    "    limpo = (separa_emoji(tudo_minusculo))\r\n",
    "\r\n",
    "    return limpo\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "source": [
    "#Transformando palavras em vari√°veis categ√≥ricas:\r\n",
    "dados['Treinamento'] = dados['Treinamento'].astype('category')\r\n",
    "test['Teste'] = test['Teste'].astype('category')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "source": [
    "dados['Treinamento']=dados['Treinamento'].apply(limpatudo)\r\n",
    "test['Teste']=test['Teste'].apply(limpatudo)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "source": [
    "ir = dados['relevancia'] == 0\r\n",
    "r = dados['relevancia'] == 1\r\n",
    "\r\n",
    "dados_r = dados.loc[r,:]\r\n",
    "dados_ir = dados.loc[ir,:]\r\n",
    "\r\n",
    "dados_ir"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>relevancia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>os roteirista de esquadr√£o suicida 2 devem ter...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>assistir esse esquadr√£o suicida 2 de marola</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e eu e gilson que fechamos a sala de cinema p ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>haterdogui esquadr√£o suicida 2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>theoncediaries ganhei 2 achei 2 envelopes t√¥ p...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>pirocona do pai √© gigante 2 games com o time f...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>romulomiranda_ tenho um monte de filmes para a...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>eu sou humano e posso sonhar homem de a√ßo 2 fo...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>1 eu ainda n√£o vi o esquadr√£o suicida ent√£o eu...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>harrymetalfan tem esquadrao suicida 2 pensei q...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>161 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Treinamento  relevancia\n",
       "1    os roteirista de esquadr√£o suicida 2 devem ter...         0.0\n",
       "2          assistir esse esquadr√£o suicida 2 de marola         0.0\n",
       "3    e eu e gilson que fechamos a sala de cinema p ...         0.0\n",
       "6                       haterdogui esquadr√£o suicida 2         0.0\n",
       "7    theoncediaries ganhei 2 achei 2 envelopes t√¥ p...         0.0\n",
       "..                                                 ...         ...\n",
       "292  pirocona do pai √© gigante 2 games com o time f...         0.0\n",
       "293  romulomiranda_ tenho um monte de filmes para a...         0.0\n",
       "295  eu sou humano e posso sonhar homem de a√ßo 2 fo...         0.0\n",
       "297  1 eu ainda n√£o vi o esquadr√£o suicida ent√£o eu...         0.0\n",
       "298  harrymetalfan tem esquadrao suicida 2 pensei q...         0.0\n",
       "\n",
       "[161 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 193
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "source": [
    "ir = test['relevancia'] == 0\r\n",
    "r = test['relevancia'] == 1\r\n",
    "\r\n",
    "test_r = test.loc[r,:]\r\n",
    "test_ir = test.loc[ir,:]\r\n",
    "\r\n",
    "test_ir"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teste</th>\n",
       "      <th>relevancia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>assisti esquadr√£o suicida 2 com um pouco de at...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vou ver esquadr√£o suicida 2 hj vcs j√° viram</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lsdcomixxx adm hj eu sonhei que ele tava no es...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>homemaranha em o esquadr√£o suicida 2 confirmad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>queria jogat lol mas quero ver esquadr√£o suici...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>aten√ß√£o acabei de assistir esquadr√£o suicida 2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>aquela cena em esquadr√£o suicida 2 que a harle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>esquadr√£o suicida 2 parece um filme de zack sn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>esquadr√£o suicida 2 vai entrar na hbo mds eu o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>vantjens esquadr√£o suicida 2 um personagem q e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Teste  relevancia\n",
       "0    assisti esquadr√£o suicida 2 com um pouco de at...           0\n",
       "1          vou ver esquadr√£o suicida 2 hj vcs j√° viram           0\n",
       "2    lsdcomixxx adm hj eu sonhei que ele tava no es...           0\n",
       "5    homemaranha em o esquadr√£o suicida 2 confirmad...           0\n",
       "6    queria jogat lol mas quero ver esquadr√£o suici...           0\n",
       "..                                                 ...         ...\n",
       "189  aten√ß√£o acabei de assistir esquadr√£o suicida 2...           0\n",
       "190  aquela cena em esquadr√£o suicida 2 que a harle...           0\n",
       "191  esquadr√£o suicida 2 parece um filme de zack sn...           0\n",
       "192  esquadr√£o suicida 2 vai entrar na hbo mds eu o...           0\n",
       "193  vantjens esquadr√£o suicida 2 um personagem q e...           0\n",
       "\n",
       "[115 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 194
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "source": [
    "dr = ''\r\n",
    "for i in dados_r.Treinamento:\r\n",
    "    dr+=i\r\n",
    "    pdr = dr.split()\r\n",
    "dir = ''\r\n",
    "for i in dados_ir.Treinamento:\r\n",
    "    dir+=i\r\n",
    "    pdir = dir.split()\r\n",
    "tr = ''\r\n",
    "for i in test_r.Teste:\r\n",
    "    tr+=i\r\n",
    "    ptr=tr.split()\r\n",
    "\r\n",
    "tir = ''\r\n",
    "for i in test_ir.Teste:\r\n",
    "    tir+=i\r\n",
    "    ptir = tir.split() \r\n",
    "\r\n",
    "\r\n",
    "ptrc = pd.Series(ptr) \r\n",
    "ptirc = pd.Series(ptir) \r\n",
    "pdrc = pd.Series(pdr) \r\n",
    "pdirc = pd.Series(pdir) \r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "dados"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>relevancia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to feliz q da p prestar aten√ß√£o e gostar de v√°...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>os roteirista de esquadr√£o suicida 2 devem ter...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>assistir esse esquadr√£o suicida 2 de marola</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e eu e gilson que fechamos a sala de cinema p ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>esquadr√£o suicida 2 √© muito bom amei o filme</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>eu sou humano e posso sonhar homem de a√ßo 2 fo...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>vi esquadr√£o suicida 2 hj com minhas irm√£ e √© ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>1 eu ainda n√£o vi o esquadr√£o suicida ent√£o eu...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>harrymetalfan tem esquadrao suicida 2 pensei q...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>nossa esquadrao suicida 2 eh tao bom q to me s...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Treinamento  relevancia\n",
       "0    to feliz q da p prestar aten√ß√£o e gostar de v√°...         1.0\n",
       "1    os roteirista de esquadr√£o suicida 2 devem ter...         0.0\n",
       "2          assistir esse esquadr√£o suicida 2 de marola         0.0\n",
       "3    e eu e gilson que fechamos a sala de cinema p ...         0.0\n",
       "4         esquadr√£o suicida 2 √© muito bom amei o filme         1.0\n",
       "..                                                 ...         ...\n",
       "295  eu sou humano e posso sonhar homem de a√ßo 2 fo...         0.0\n",
       "296  vi esquadr√£o suicida 2 hj com minhas irm√£ e √© ...         1.0\n",
       "297  1 eu ainda n√£o vi o esquadr√£o suicida ent√£o eu...         0.0\n",
       "298  harrymetalfan tem esquadrao suicida 2 pensei q...         0.0\n",
       "299  nossa esquadrao suicida 2 eh tao bom q to me s...         1.0\n",
       "\n",
       "[300 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 195
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "source": [
    "fpdr=pdrc.value_counts(True)\r\n",
    "fpdir=pdirc.value_counts(True)\r\n",
    "fptr=ptrc.value_counts(True)\r\n",
    "fptir=ptirc.value_counts(True)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "source": [
    "listatr = ptrc.tolist()\r\n",
    "listatir = ptirc.tolist()\r\n",
    "listadr = pdrc.tolist()\r\n",
    "listadir = pdirc.tolist()\r\n",
    "lista_r = listatr + listadr\r\n",
    "lista_ir = listatir + listadir\r\n",
    "lista_total2 = pd.Series(lista_r + lista_ir)\r\n",
    "\r\n",
    "lista_total1 = pd.Series(listadr+listadir)\r\n",
    "\r\n",
    "Banco_de_dados=set(listadr+listadir)\r\n",
    "\r\n",
    "\r\n",
    "lista_total1.value_counts()\r\n",
    "\r\n",
    "print(Banco_de_dados)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'grandes', 'porrada', 'oesquadr√£osuicida', 'film', 'maneirona', 'sobre', 'florzinha', 'prazeres', 'estrela', 'at', 'sebastian', 'tao', 'kkkkkkkkk', 'p√≥s', 'httpstcoxtjfiizobdestou', 'bob√£o', 'fodasepirocona', 'superou', 'riot', 'hora', 'httpstcodvu9ldva27top10', 'tentando', 'novomymaddiecade', 'chato', 'duvido', 'rirfilmes', 'melhorq', 'mkkkkkkkkmksobre', 'metadeque', '10', 'escreverassistir', 'bomizzyunou', '9', 'este', 'guerreira', 'decepcionadoesquadr√£o', 'as', 'onde', 'eternos', 'm√©esquadr√£o', 'deveria', 'filmes', 'idade', 'fico', 'gunn', 'guerra', 'ver', 'dcmds', '1pedrokkkkk', 'shangchi', 'lan√ßa', '2after', 'neg√≥cios', 'bomesquadr√£o', 'cortar', 'arruinando', 'oq', 'prestar', 'cabe√ßa', 'cena', 'tooth', 'irm√£', '1', '2698', 'cidade', '2assistir', 'massacrou', 'ator', 'amei', 'rumor', 'franquia', 'bichoassisti', 'marvel', 'cmg', 'infestada', 'rosto_implorandosim', 'tiram', 'absurdo', 'rosto_chorando_de_rireu', '6esquadr√£o', 'favorita', 'escuto', 'comedia', 'hater', '25set', 'significa', 'fogo', 'justamente', 'legendado', 'kkkkkkkkkkkkkkkkkkkk', 'heroi', 'pprt', 'pago', 'tequila', 'anuncio', 'ruindademeu', 't√¥', 'httpstcoyjrzd4g2vkeu', 'antes', 'milh√µes', 'faria', '9nem', 'falando', 'kkk', 'nos', 'maior', 'dez', 'ganha', 'httpstcolgnjorkwxoe', 'toca', 'vai', 'rosto_expressando_desagrado', 'viu', 'gastodigo_horacio', '2alerquina', 'lan√ßou', 'buzina', 'alvo', 'imaginar', 'quinn', 'ruim‚ö†', 'detetive_mulher_pele_morenaesquadr√£o', 'zzz', 'suicidesquad2', 'vaimuito', 'dnv', 'leptospiroseiaewk', 'n√©alta_tens√£o', 'sozinhavi', 'edit', 'melhoresquadr√£o', 'apesar', 'mosc√£oteiros', 'todo', 'kkkkkkk', 'partes', 'violadavis', 'batman', 'saboroso', 'httpstcozhzfbgpjupbem', 'legalquem', 'alguma', 'fora', 'noooromulomiranda_', 'kinh__', 'exibidos', 'assim', 'se', 'patrulha', '2depois', 'super', 'httpstcovnpavh1elhluafengarie', '74m‚Ç¨', 'ratcatcher', 'ca√ßa', 'iria', 'evagelion', 'estrelada', 'chi', 'foram', 'wanda', 'gravaram', 'polka', '2vei', 'aleluiapor', 'fic√ß√£o', 'incendiou', 'carinhosamente', 'verde', 'ajudar', 'tenso', 'falta', 'os', '√≥tima', 'cartoonstava', 'hjoxentepipoca', 'poucoa', '70', 'trash', '6', '‚ù§', 'lindalilmarks1', 'todomundoodeiaochris', 'aquela', 'ano', 'ilhadenot√≠cias', 'bom', 'chocada', 'tiro', 'poderoso', 'tamb√©m', 'superior', 'esses', 'respires', 'n√£o', 'hero√≠na', '1filme', 'tubar√£o', 'ponto', 'com√©dia', 'pernambucana', 'rosto_dormindoesquadr√£o', 'falaremos', 'demorado', 'httpstcounpzka0x8teu', 'mem√≥riavai', 'trincaqueria', '2esquadr√£o', 'brabo', 'news', '1boss', 'gal√°xia', 'parte', 'horaspensando', '2theoncediaries', 'por', 'qualidade', 'snyder', 'dceu', 'melhor', 'demaisaquele', 'otimo', 'setembro', 'todoeu', 'building', 'perde', 'day', 'sou', 'vez', 'quero', 'pqpassistam', 'kombat', 'honestamente', 'dccomics', 'vi', 'gnt', 'prettylittleliars', 'boca', '√©', 'burrice', 'carrinho_de_compras', 'cartaztop10', 'tristesesquadrao', 'ter+', 'uma', 'rosto_sorridente_com_olhos_de_cora√ß√£ooq', 'terminei', 'quarto', 'crimea', 'locoggukchase', 'paulada', 'seguidos', 'nota_de_d√≥lar', 'serem', 'quarta', 'primeiroacabei', 'filme', 'lugar', 'consegue', 'velozes', 'ingresso', 'numa', 'nova', 'reassistir', 'rei', 'desperdi√ßaram', 'worst', 'vc', 'rosto_sorridente_com_3_cora√ß√µese', 'for√ßa', 'rosto_sorridente_com_3_cora√ß√µes', 'ultimato', 'assistindo', 'seus', 'rosto_nauseadonossa', 'assassinos+yasutoky', 'nanaui', 'julietteblackpinkvotees', 'saiba', 'aquelas', 'justi√ßa', 'silencioso', 'vidapra', 'rosto_com_olho_piscando', 'rato', 'gloriagroove', 'fui', '3', 'bizarro', 'm√£os_juntas_pele_clarateline', 'ir√°', 'murders', 'salvar', 'lembrar', 'fofoesquadr√£o', 'shanguinho', 'tambem', 'unico', '10era', 'sinal', 'cleo', 'dublada', 'karol', 'realmente', 'tr√™s', 'promo', 'visison', 'lady', '√≥timo', 'ratinhos', 'interessar', 'conk√°', 'livros', 'httpstcohjlqydin9qe', 'tanto', '01', 'gra√ßaesquadr√£o', '3patrulha', 'aneis', 'ia', 'palavreado', 'espero', 'arlequinha', 'pediu', 'd√∫vida', 'fotos', 'ai', '√≥', 'isso¬ø¬ø¬ø‚ÄΩ‚ÄΩesquadr√£o', 'coringacornetapalestr', 'fodaterminei', 'estreia', 'importavendo', 'piratamubibrasil', 'incr√≠vel', 'humano', 'forte', 'milllllll', 'fuiprecisamos', 'pelo', 'szkntos', 'pega', 'caminhos', 'viera', 'assustade', 'sabiaeu', 'bostatem', 'dubla', 'ruins', 'temo', 'ruim', 'dessa', 'Ô∏èspoiler', 'sabe', 'novos', 'victoria', 've', 'suic√≠da', 'recente', 'abordaremos', 'bosta', 'maneiro', '‚Ä¢', 'starro', 'Ô∏ècelestvics', 'geral', 'kkkkkkkkkk', 'pqp', 'palha', 'adolescente', 'parar', 'httpstcopwsse0q2p8logan', 'estar√°', 'jack', 'cr√©dito', 'canina', 'filmeizabelardg', 'muuuito', '‚ö†', '29', 'assiste', 'kkkkkkkkkkkkkkkk', 'gabopantaleaov√©i', '4', 'opini√µes', 'mto', 'tudo', 'vazio', 'torta', 'thesuicidesquad', 'gosto', 'assisti', 'arlequina', 'que', 'amooooookageycat', 'vale', 'httpstcogciymn5lzlalgu√©m', 's√©rie', 'wual', 'criado', 'saindo', 'foda', 'depressaoesquadr√£o', 'nesse', 'noite', 'httpstcolk5ktnokoadavidbqwie', 'celular', 'sess√£o', 'promo√ß√£o', 'agorasegundo', 'cinefila', 'episodio', 'carregou', '35‚òÖ', 'com', 'demaisesquadr√£o', 'furiosos', 'kkkkkesquadr√£o', 'conpletamente', 'ent', '136', 'poderosa', '160', '2‚Ä¶‚Ä¶', 'olha', 'maluco', 'alice', 'classificativa', 'n√©glr', 'entendereu', 'gastei', 'crian√ßas', 'baby', 'kkkkk5', 'primeiroa', '2016', 'nem', 'dei', 'dc', 'ta', 'dilma', 'p√°', 'malesquadr√£o', 'chamo', 'nanue', 'ca√ßaratos', '2o', 'altura', 'tava', '2pra', 'rosto_sorridente_com_√≥culos_escuros', 'picsart', 'avataralendadeaang', 'pisou', 'data', '2at√©', 'esquadaro', 'esmaga', 'selecionou', 'grande', 'caramba', 'seres', 'breve', 'pqpte', 'carey', 'camiseta', 'devem', 'falou', 'gostinho', 'demorei', 'jogar', 'suicidesquad2esquadr√£o', '7candyman', 'pro', 'tem', 'bizarras', 'falandoviolentcrims', 'entre', 'final', 'dia', 'pedrocertezas', '2vo', 'sorreio', 'rec√©mchegados', 'umas', 'humanos', 'superar', 'httpstco5wv9gje0itfilmes', '2a', 'constatar', 'gosta', 'httpstcoitczgcjub0', 'toda', 'top10moviestxtpenis', 'viram', 'nerdboomer', 'eweu', 'primeiros', 'homi', 'eles', 'acompanhadomargot', 'kkkkkkkminha', 'kkkpueblakleydson', 'httpstcoqdcgrtdyfpspoiler', 'horrores', 'frango', 'para', 'ja', 'morrer', 'queen', 'veremos', 'melhores', 'trilha', 'bilheteria', 'bommmcarai', 'julgueesquadr√£o', 'nome', 'document√°rio', 'ainda', 'esquadrao', 'isso_leazy', 'bomadudmaia', 'n√£oinmydrew', 'dela', 'httpstcouayv6dyjbq', '4esquadr√£o', 'casa', 'vingadores', 'james', 'tal', 'cenas', 'outra', 'baixagostei', 'n√£oesquadrao', '164', 'problemas', 'insiste', 'pela', 'hypados', 'aguardo', 'aquaman', 'cheguei', 'coletivo', 'lenda', 'muitotodo', '1wooyngoth', 'editado', 'series', '8mist√©rio', 'httpstcotpjc90fxwq1', 'httpstcotbktnxjmdxvitorsfc88', 'tvcinema', 'mortal', 'por√©m', 'gostar', '2free', 'p√©', 'seria', 'achei', 'piadinha', 'perfeito', 'matando', '√©esquadr√£o', 'sucesso', 'produzindo', 'bomba', 'inflitrado', 'representatividade', 'abnerkrill', 'esperando', 'notimetodie', 'marcado', 'aves', 'httpstconyxuncm39rapagando', 'era', 'free', 'trata', 'lix√£o', 'pirataria', 'gomez', 'samurai', 'kkkk', 'restaurante', 'top', 'player', 'prr', '2daniel123desa', 'dono', 'achando', 'httpstco8focn5w6rc', '5', '√∫ltimo', 'recentemente', 'pata', 's√≥', 'mima', 'httpstcoljgtzwynrpmanooooo', 'dias', 'in', 'acaso', 'ningu√©m', 'crlesquadr√£o', 'encantado', 'seja', 'brasileira', 'only', 'chegam', 'httpstcoi2i7gsswhpo', 'dentro', 'hoje', 'sangue', 'quem', 'rever', 'latina', 'del√≠rio', 'personagem', 'crushdei', 'isso', 'seriamente', 'timegduvivier', 'sweet', 'top10moviesde', 'nn', 't√£otop', \"6d'artac√£o\", 'genero', 'nota', '90', 'pr√©', 'salva', 'encheu', 'posso', 'aquilo', 'programa', 'httpstcotylvony0qa', 'ow', 'me', 'meter', 'criminal', '9a', 'man', 'httpstcoqvlchkpkbdat√©', 'grooveesquadr√£o', 'come√ßa', '‚Äúo', 'httpstcox3iolxrmiwhbomaxbr_portal', 'alguns', 'httpstcosgvodey2pemano', 'httpstcopecbg6t7ajfui', 'acabei', '1¬∞', 'pqpesquadr√£o', 'nao', 'chata', 'selena', 'esp√©cie', 'tendo', 'ramo', 'completa', 'protegida', 'descobri', 'lan√ßar', 'cueca', 'r2206', 'estranho', 'httpstco1habg69eyeharrymetalfan', 'httpstcocxyvxct1deesquadr√£o', 'achandoesquadr√£o', 'afirmou', 'emesquadr√£o', '‚ô°', 'ajude', 'ap√≥s', 'algumas', 'd', 'action', 'portugu√™sassisti', 'parecendo', '‚Äúa', 'knd', 'colapso', 'sai', 'apaixonada', '3boss', 'comprovante', 'kamila', 'rosto_ruborizado', 'sie', 'zeldinhafui', 'bomnossa', 'kkkkkkmarcusrocha', 'violencia', 'liga', '2top', 'fazer', 'desenhos', 'l√°', 'apenas', 'qualquer', 'rosto_chorando_de_rir', 'cruise', 'robbie', 'caralhoosetembro', 'httpstco8os9nhho9ecinema', 'completamente', 'roteirista', 'httpstcoyychedtc1fvou', '5free', 'descobrir', 'fofos', 'junto', 'aqui', 'fant√°stico', 'v√©i', 'bastante', 'atrizei', 'americano', '50', 'tok', 'bar', 'achar', 'httpstcoaxxvi8yaguesquadr√£o', 'horr√≠vel', 'terminando', 'livro_aberto', 'esquadr√£o', 'voz', 'ca', 'duas', 'obra', 'httpstcod1npcwm4ufto', 'doninha', 'arte', 'gigante', 'venom', 'rindo', 'filmemeu', 'forma', 'indicou', 'hq', 'desnecess√°rias', 'surrou', 'ca√≠', 'peacemaker', 'solto', 'porra', '2015', 'katana', 'a', 'vamos', 'refletindo', 'puder', 'chocado', 'rosto_implorando', 'vermelho', 'imperialismo', 'filmeesquadr√£o', 'nada', 'aparece', 'sexualidade', 'spinoff', 'mesma', 'criticar', 'algum', 'funcionar√°', 'boomerang', 'suicida', 'comum', 'vi√∫va', 'conta', '2cara', 'das', 'thr', 'passaram', 'todos', 'primeira', 'assistir', 'agora', 'm√∫sica', 'tocar', 'termos', 'cr√≠tica', 'num', 'craig', 'mtvmiawfandomblink', 'podre', 'longe', 'milh√£o', 'morte', 'Ô∏èpipoca', 'dele', 't√°', 'envelopes', 'cara', 'repente', 'sinceramente', 'Ô∏è', 'd√≠vida', 'linda', 'principal', 'unicamente', 'httpstcogjrnqdqctpromulomiranda_', 'sexta', 'rolando_no_ch√£o_de_rirmtt', 'rosto_chorando_de_rirdo', 'httpstcoggii9gntcv', 'expectativas', 'foi', 'httpstcozt5k6osvtagloria', 'dirigiu', 'premiosmtvmiaw', '55', 'internet', 'beleza', 'pq', 'cinema', 'parece', 'ultimamente', 'mandar', 'no', 'passar', 'problema', 'come√ßo', 'de', 'odioesquadr√£o', 'suicida2', 'agoraacabei', 'amanh√£', 'depois', 'httpstcoe99lfeue8ukimbeerly_02', 'dublado', 'aranha', 'produtoraesquadr√£o', 'monte', 'mulher', 'bompoderia', 'ou', '2acabei', 'vers√£o', 'malditas', 'rosto_fazendo_sinal_de_sil√™ncio', 'ent√£o', 'falar', 'um', 'q', 'memeontem', 'diverti', 'noiapra', 'kct', 'mds', 'elemento', 'desde', 'pare√ßo', 'pacificador', 'estamos', 'irma', 'como', '60', 'tl', 'tarde', 'costaaaaaaasnunca', 'httpstcoy85mi6eyuemano', 'fofotriste', 'enfim', 'divertido', 'merecia', 'httpstcofntxc7xjmewalfritsch', 'expectativa', 'background', 'perdi', 'and', 'parab√©ns', 'teve', 'lembro', 'pare√ßa', 'hein', 'pipocavegana', 'sonhar', 'give', 'verdadeira', 'jonhcena', 'combinar', 'daddy', 'porcaria', 'dora', 'pra', '1shangchi', 'space', '2assisti', 'mn', 'issues', 'faz', 'furios', 'passando', 'gente', 'matar', 'fez', 'auge', 'o', 'putt', 'p√©ssimo', 'uns', 'max', 'pipoca', 'slk‚ô°', 'mortaissavitargod333', 'fraqu√≠ssimo', 'indo', '7', 'br', 'acho', 'usou', 'ratoesquadr√£o', 'inteiro', 'aten√ß√£o', 'anima', 'httpstcoeuxdvthonmvi', 'essa', 'mtvmiawhitlovesickgirls', 'rosto_ruborizadoquero', 'personagens', 'dot', 'daqui', 'outro', 'jun√ß√£o', 'vera', '10velocidade', 'altas', 'supera', 'entrar', 'rosto_implorando2', 'margot', 'valeu', 't√£o', 'dois', 'le√£o', 'sido', 'vendo', 'creditoseu', 'shang', 'hbomax', 'colocar', 'escolher', 'particularmente', '8', 'ir', 'fim', 'serio', 'rosto_chorando_aos_berrosnunca', 'now', 'passado', 'passou', 'guardi√µes', 'fosse', 'comida', 'ser√°', 'bi', 'coisas', 'desse', 'm√£o_em_v_de_vit√≥ria_pele_claravou', 'branca', 'entregou', 'cient√≠ficaihugkugisaki', 'fiquei', '√≥bvio', '80', 'moral', 'maravilhosa', 'after', 'necessito', 'mariah', 'qu√£o', 'braga', 'gostei', 'deizao', 'pessoal', 'pai', 'sua', 'comprei', 'a√≠', 'futuro', 'gabo', 'prometeu', 'recomendotava', 'queria', 'c', 'httpstcofuf60dflx4velozes', 'mera', '2021', 'ter', 'negra', 'lago', 'tml', 'legado', 'interagir', 'momentosacabei', 'favorito', 'rainhas', 'esperar', 'ficou', 'olhei', 'httpstcosalo5vig3zesquadr√£o', 'fato', 'chorar', 'imperfei√ß√µes', 'estavam', 'ksksksksksks', 'infinita', 'mt', 'a√ßo', 'gun', 'semanas', '710', 'eu', '2fase', 'legaisquero', 'colho', '007', 'arteassisti', 'suicidesquad', 'her√≥i', 'ao', 'groove', 'pensei', 'vadias', 'carvalho', 'bebido', 'crime', 'piadas', 'projeto', 'vil√£o', 'rapina', 'eli', 'ilhasolteira', 'comento', 'fofo', 'feliz', 'doin', 'nanaue', 'pr√≥pria', 'pior', '2esse', '1gostei', 'balangarcadeladaharley', 'imposs√≠velliarodriguex', 'sequ√™ncia', \"hq's\", '+16', 'sharkassisti', 'ajudananathhs', 'motivo', 'fiz', 'fundo', 'formado', '2n√£o', 'anos', 'tocando', 'amigo', 'estiver', 'man√©', 'voc√™', 'assistiu', 'sainttropez', 'sentindo', 'seriogabrielnerdland', '98m‚Ç¨', 'querendo', 'quando', 'pr√≥ximo', 'poder', 'ate', 'mundialmente', 'distra√≠da', 'claro', 'ratos', 'ruimesquadr√£o', 'httpstco43yw2yaxfkjuliettefinfos', 'arlequinaassisti', '7a', 'httpstcoqlmfpewnu5top10', 'simfrleticya', 'tipo', 'del', 'm√≠nimo', 'fa', 'jogando', 'simplesmente', 'rey', 'e', 'daniel', 'd√≥', 'deadpool', 'preferi', '2indo', 'eram', 'm√£o_em_v_de_vit√≥ria', '41m‚Ç¨aquela', 'diz', 'nordeste', 'bras√≠lia', 'n', '2', 'provavelmente', 'dedo', '5candyman', 'apaixonado', 'trevas', 'fala', 'entrega', 'propositalmente', 'qual', 'david', 'ele', 'arqueiro', 'candyman', \"8d'artac√£o\", 'arlequinaquem', 'evolu√≠da', 'pouco', 'shot', 'maravilhoso', 'esquadraosuicida', 'desencontro', 'morre', 'gracinha', 'sem', 'httpstcox5csi9xixet√¥', 'preferia', 'rosto_sorridente_com_olhos_de_cora√ß√£o', 'casafavsmcavoy', 'mim', 'bem', 'pois', 'alta_tens√£o', 'httpstcoecf8fzqmpns√©rio', 'refer√™ncia', 'no√ß√£o', '2ah', 'mdsque', 'httpstcoou0u5lra7wesquadr√£o', 'pre', 'estilo', 'time', 'gl√≥ria', 'pos', 'kkkƒ∑kkk', 'apx', 'explica√ß√£o', 'etmr_antoniojr', 'minhas', '2017', 'pqpri', 'capit√£o', 'jonh', '2que', 'adorei', 'faltando', 'apelidei', 'saiu', 'bolinha', 'finalmente', 'chiquero', 'algu√©m', 'tomate', 'primeiro', 'brutal', 'santos', 'fam√≠lia', 'conseguir', 'est√£o', 'sala', 'maxduduzera25', 'conferir', 'meio', 'porque', 'marolae', 'finalsem', 'opini√£o', 'amo', 'empadinhas', 'piores', '2¬∞', 'bostari', 'continuam', 'chatokkkkfui', 'maratonar', 'sair', 'primeiroesquadr√£o', 'hbomaxbr', 'clime', 'hora‚Ä¶', 'puta', 'estou', 'rosto_de_cabe√ßa_para_baixo', '‚Äúvil√µes', 'm√™s', '√∫nica', 'jam', 'jeito', 'incr√≠velvou', 'remake', 'v4', 'it', 'tela', 'gloria', 'espelho', 'conseguiria', 'dif√≠cil', 'sabia', '4tive', 'meses', 'merece', '2enfim', 'harrypotter', 'fantasy', 'kkkkkk', 'em', 'link', 'podcast', 'santosfc', 'mas', 'nenhum', 'dubl√™', 'fd', 'alerquina', '15', 'rosto_dormindo', 'esperava', 'simesquadr√£o', 'vision', 'fechamos', 'exatamente', 'tik', 'coringa', 'assisto', 'minutos', 'sacoesquadr√£o', 'mil', 'est√°', 'furiosa', 'dos', 'inesperada', 'guy', 'emocionadaaaa', 'ruimquando', 'polkadotman', 'primeiro‚Ä¶', 'vida', 'chave', 'sorry', 'cora√ß√£o_crescendo', '4patrulha', 'at√©', 'apareceu', 'chorei', 'am√©rica', 'imagem', 'tr√™shoje', 'mais', 'muuuuito', 'everybodyhateschris', 'ela', 'depende', 'live', 'convictaamei', 'gilson', 'agitado', 'recentes', 'coisa', 'prestando', 'cine', 'cadelinha', 'filmehenriquenarizz', 'j√°', 'apaixonada√©‚Ä¶‚Ä¶cara', 'httpstcogemtnxbekapacificador', 'na', 'sangrento', 'pariu', 'amoesquadr√£o', '2t√¥', 'passada', 'dar', 'vente', 'ser', 'passarinhosesquadr√£o', 's√©tima', 'mes', 'primeironossa', 'do+', 'nuvem', 'bons', '75', '2resumo', 'pegando', 'p√°ginas', '2ok', 't√™m', 'entendo', 'deve', '17478383', 'mts', 'odiooo', 'atoa', 'tirando', 'povo', 's√©rio', 'an√©is', 'nunca', 'infiltrado', 'aventureira', 'agr', 'sendo', 'enredo', '9chal', 'ep', 'davidson', '90m‚Ç¨', 'arrecadou', 'minds', 'hj', 'vol', 'httpstcoxq3e9z00jskashruno', 'k', 'tempo', 'ben', 'vou', 'littlecansada', 'n√©e', 'hbo', '44', 'oscar', 'quanto', 'drive', 'absolutamente', 'agoraruanfalco', 'morrido', 'dinheiro', 'httpstcoyltb4wlzmjhaterdogui', 'deu', 'revendo', '1esquadr√£o', 'sobrinha', 'mesmo', 'amazon', 'caralhos', 'pessoas', 'chega', 'sera', 'stallone', 'ratopessoal', 'realminha', 'venham', 'membros', 'nossaaaa', 'pete', 'vcsgabixferrars', 'obriga√ß√£o', '16', 'hist√≥ria', 'chefinho', 'helena', 'feedbacks', 'harley', 'certeza', 'meia', 'kkkkkkkkkkkkkkkkkkkesquadr√£o', 'mundo', 'tinha', 'fica', 'quinta', 'muito', 'boas', 'google', 'mal', 'ganhamos', 'n√©passado', '2quero', 'solo', 'menos', 'te', 'to', 'demais', '2iriam', 'fant√°sticas', 'tentativa', 'gore', 'tenho', 'homem', 'john', 'chin√™s', 'pena', 'dcne3djimi', 'viol√™ncia', 'salas', 'berna', 'nham', 'do', 'games', 'leticya', 'the', 'seguidas', 'da', 'observa√ß√µes', 'jungle', 'primeiroca√ßa', 'min', 'fodase', 'blackpinkheyalviverde', '√∫ltimosshang', 'dcvoc√™s', '230coisas', 'sempre', 'tudoachei', 'louca', 'tristeesquadr√£o', 'acontecer', 'terem', 'nossa', 'causa', 'domingo', 'merda', 'talvez', 'escolheu', '500', 'vote', 'diretorgostei', 'eh', 'idiota', 'esmurra', 'tempos', 'retorno', 'off', 'chorando', 'podia', 'ontem', 'dmsssssss', 'nas', 'httpstcojwhnlt6fuxeu', '12', 'quer', 'juntou', 'homem_decepcionado_pele_morena_escuraesquadr√£o', 'focar', 'ali', 'filmet√°', 'interpretado', 'httpstcogxptz821ueesquadr√£o', 'httpstcobc50acicdwesquadr√£o', 'pprtn√£o', 'minha', 'legal', 'cansativa', 'matths_az', 'semana', 'dizer', 'üìΩ', 'i', 'abertoesquadr√£o', 'nessa', 'esse', 'kskwkdkwjdskeu', 'poss√≠vel', 'pregui√ßa', 'tss', 'tu', 'lan√ßamento', 'dcfandome', 'estava', 'tarantino', 'vsf', 'ayer', 'v√°rios', 'suicidat√¥', 'meu', 'sobreesquadr√£o', 'boss', '2tava', 'olhando', 'coelho', 'timeline', 'convencional', '140', 'httpstcolstrxvtaghesquadr√£o', 'sei', 'tbm', 'sonora', 'p', 'gaga', 'pedro', 'horrivel', 'm√≥', 'tempo√©', 'lana', 'reapari√ß√£o', 'v√°rias', 'conseguiu', 'us', 'cor', 'anivers√°rio', 'novo', 'sosto', 'kkkkkkkkkmds', 'legalgente', '2finalmente', 'suicidao', 'olhospqp', 'neleesquadr√£o', 'jumanji', 'volumes', 'descendo', 'ri', 'viassistindo', 'deus', 'bommesquadr√£o', 'aclamada', 'puto', 'shazam', 'come√ßou', '√¥', 'quadrinhos', 'assitiu', 'erros', '61m‚Ç¨', 'sim', 'vezes', 'mortes', 'amando', 'ganhei', 'assistamamei', 'pessoa', 'ganhou', 's√£o', 'pdrbnt', '21', 'lan√ßados'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "source": [
    "# probabilidade de ser relevante:\r\n",
    "pr = len(listadr)/len(lista_total1)\r\n",
    "# probabilidade de ser irrelevante:\r\n",
    "pi = len(listadir)/len(lista_total1)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "print(pr+pi)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "source": [
    "def prob_R_I(text):\r\n",
    "    Prob1 = 1\r\n",
    "    Prob2 = 1\r\n",
    "    for i in text.split():\r\n",
    "        fr = fpdr[i]\r\n",
    "        fir = fpdir[i]\r\n",
    "        Alaplace_relevante = (fr+1)/(len(listadr)+len(Banco_de_dados))\r\n",
    "        Alaplace_irrelevante = (fir+1)/(len(listadir)+len(Banco_de_dados))\r\n",
    "        Prob1 *= Alaplace_relevante\r\n",
    "        Prob2 *= Alaplace_irrelevante\r\n",
    "    if Prob1*pr > Prob2*pi:\r\n",
    "        return 1\r\n",
    "    else: \r\n",
    "        return 0 \r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "source": [
    "print(prob_R_I('haterdogui esquadr√£o suicida 2'))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'haterdogui'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2894\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2895\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'haterdogui'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-205-a4928725d61d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob_R_I\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'haterdogui esquadr√£o suicida 2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-199-e94640e6b4d7>\u001b[0m in \u001b[0;36mprob_R_I\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mProb2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mfr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfpdr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mfir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfpdir\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mAlaplace_relevante\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfr\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlistadr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBanco_de_dados\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 882\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[1;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 989\u001b[1;33m         \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    990\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2895\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'haterdogui'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def freq_absoluta(palavra,freq_absoluta_conjunto):\r\n",
    "    \"\"\"\r\n",
    "    Conta quantas vezes determinada palavra apareceu \r\n",
    "    na respectiva categoria, seja relevante ou irrelevante\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    try:\r\n",
    "        return freq_absoluta_conjunto[palavra]\r\n",
    "    \r\n",
    "    except:\r\n",
    "        return 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "frase = 'esquadrao suicida √© muito bom'\r\n",
    "n = 1\r\n",
    "for i in fpdir[frase.split()]:\r\n",
    "    n *= i \r\n",
    "Probpdir = n   \r\n",
    "print(Probpdir)\r\n",
    "\r\n",
    "n = 1\r\n",
    "for i in fpdr[frase.split()]:\r\n",
    "    n *= i \r\n",
    "Probpdr = n   \r\n",
    "print(Probpdr)\r\n",
    "\r\n",
    "n = 1\r\n",
    "for i in fptr[frase.split()]:\r\n",
    "    n *= i \r\n",
    "Probfptr = n   \r\n",
    "print(Probfptr)\r\n",
    "\r\n",
    "n = 1\r\n",
    "for i in fptir[frase.split()]:\r\n",
    "    n *= i\r\n",
    "Probfptir = n   \r\n",
    "print(Probfptir)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.3611458083278806e-12\n",
      "1.5299426747422336e-09\n",
      "1.834138924277446e-09\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "\"Passing list-likes to .loc or [] with any missing labels is no longer supported. The following labels were missing: Index(['bom'], dtype='object'). See https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\"",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-342c18574806>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfptir\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfrase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mn\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mProbfptir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    904\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 906\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    907\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_with\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m         \u001b[1;31m# handle the dup indexing case GH#4246\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_values_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    877\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 879\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1097\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot index with multidimensional key\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m             \u001b[1;31m# nested tuple slicing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m         \u001b[1;31m# A collection of keys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m         \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m         return self.obj._reindex_with_indexers(\n\u001b[0;32m   1039\u001b[0m             \u001b[1;33m{\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1255\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0moption_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"display.max_seq_items\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"display.width\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1315\u001b[1;33m                     raise KeyError(\n\u001b[0m\u001b[0;32m   1316\u001b[0m                         \u001b[1;34m\"Passing list-likes to .loc or [] with any missing labels \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1317\u001b[0m                         \u001b[1;34m\"is no longer supported. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Passing list-likes to .loc or [] with any missing labels is no longer supported. The following labels were missing: Index(['bom'], dtype='object'). See https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\""
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "### Verificando a performance do Classificador\n",
    "\n",
    "Agora voc√™ deve testar o seu classificador com a base de Testes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "### Concluindo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "### Qualidade do Classificador a partir de novas separa√ß√µes dos tweets entre Treinamento e Teste\n",
    "\n",
    "Caso for fazer esse item do Projeto"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "## Aperfei√ßoamento:\n",
    "\n",
    "Trabalhos que conseguirem pelo menos conceito B v√£o evoluir em conceito dependendo da quantidade de itens avan√ßados:\n",
    "\n",
    "* IMPLEMENTOU outras limpezas e transforma√ß√µes que n√£o afetem a qualidade da informa√ß√£o contida nos tweets. Ex: stemming, lemmatization, stopwords\n",
    "* CORRIGIU separa√ß√£o de espa√ßos entre palavras e emojis ou entre emojis e emojis\n",
    "* CRIOU categorias intermedi√°rias de relev√¢ncia baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante. Pelo menos quatro categorias, com adi√ß√£o de mais tweets na base, conforme enunciado. (OBRIGAT√ìRIO PARA TRIOS, sem contar como item avan√ßado)\n",
    "* EXPLICOU porqu√™ n√£o pode usar o pr√≥prio classificador para gerar mais amostras de treinamento\n",
    "* PROP√îS diferentes cen√°rios para Na√Øve Bayes fora do contexto do projeto\n",
    "* SUGERIU e EXPLICOU melhorias reais com indica√ß√µes concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "* FEZ o item 6. Qualidade do Classificador a partir de novas separa√ß√µes dos tweets entre Treinamento e Teste descrito no enunciado do projeto (OBRIGAT√ìRIO para conceitos A ou A+)"
   ],
   "metadata": {},
   "attachments": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "## Refer√™ncias"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "978d606978bb26dc1196d980de5f982f9f8ebb0565a7cd0735526ac935a81f1b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}