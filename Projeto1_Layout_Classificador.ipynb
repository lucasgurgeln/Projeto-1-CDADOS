{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Projeto 1 - Ciência dos Dados"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nome: João Pedro Reis Lima\r\n",
    "\r\n",
    "Nome: Lucas Gurgel "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Atenção: Serão permitidos grupos de três pessoas, mas com uma rubrica mais exigente. Grupos deste tamanho precisarão fazer um questionário de avaliação de trabalho em equipe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "Carregando algumas bibliotecas:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "source": [
    "%matplotlib inline\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "source": [
    "print('Esperamos trabalhar no diretório')\r\n",
    "print(os.getcwd())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Esperamos trabalhar no diretório\n",
      "c:\\Users\\lucas\\Downloads\\Projeto-1-CDADOS\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Carregando a base de dados com os tweets classificados como relevantes e não relevantes:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "source": [
    "filename = 'esquadrão suicida 2.xlsx'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "source": [
    "dados = pd.read_excel(filename)\r\n",
    "dados.head(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>relevancia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to feliz q da p prestar atenção e gostar de vá...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>os roteirista de esquadrão suicida 2 devem ter...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>assistir esse esquadrão suicida 2 de marola</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e eu e gilson que fechamos a sala de cinema p ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>esquadrão suicida 2 é muito bom, amei o filme</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Treinamento  relevancia\n",
       "0  to feliz q da p prestar atenção e gostar de vá...         1.0\n",
       "1  os roteirista de esquadrão suicida 2 devem ter...         0.0\n",
       "2        assistir esse esquadrão suicida 2 de marola         0.0\n",
       "3  e eu e gilson que fechamos a sala de cinema p ...         0.0\n",
       "4      esquadrão suicida 2 é muito bom, amei o filme         1.0"
      ]
     },
     "metadata": {},
     "execution_count": 218
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "source": [
    "test = pd.read_excel(filename, sheet_name = 'Teste')\r\n",
    "test.head(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teste</th>\n",
       "      <th>relevancia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>assisti esquadrão suicida 2 com um pouco de at...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vou ver esquadrão suicida 2 hj, vcs já viram?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@lsdcomixxx adm hj eu sonhei que ele tava no e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ranking personagens do esquadrão suicida:\\n\\n1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mano gloria groove no esquadrão suicida 2 fico...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Teste  relevancia\n",
       "0  assisti esquadrão suicida 2 com um pouco de at...           0\n",
       "1      vou ver esquadrão suicida 2 hj, vcs já viram?           0\n",
       "2  @lsdcomixxx adm hj eu sonhei que ele tava no e...           0\n",
       "3  ranking personagens do esquadrão suicida:\\n\\n1...           1\n",
       "4  mano gloria groove no esquadrão suicida 2 fico...           1"
      ]
     },
     "metadata": {},
     "execution_count": 219
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "## Classificador automático de sentimento\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nós consideramos relavante comentários que demostravam alguma critica ao produto tais como: Se a pessoa gostou ou não da obra, o que ela sentiu(emoção), também consideramos personagens favoritos como algo relavante \r\n",
    "\r\n",
    "Não consideramos comentarios de situações adversas sem relação direta com a criticidade da obra, tal como comentarios pessoais"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "### Montando um Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "source": [
    "import re \r\n",
    "import emoji\r\n",
    "from emoji import UNICODE_EMOJI\r\n",
    "\r\n",
    "\r\n",
    "def cleanup(text):\r\n",
    "    \"\"\"\r\n",
    "        Função de limpeza muito simples que troca alguns sinais básicos por espaços\r\n",
    "    \"\"\"\r\n",
    "    #import string\r\n",
    "    punctuation = '[”@\\-/!.:?;,''\"|()#$%¨&*]' # Note que os sinais [] são delimitadores de um conjunto.\r\n",
    "    pattern = re.compile(punctuation)\r\n",
    "    text_subbed = re.sub(pattern, '', text)\r\n",
    "    return text_subbed"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "source": [
    "def minusculo(text):\r\n",
    "    return text.lower()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "source": [
    "def separa_emoji(tweet):\r\n",
    "   \r\n",
    "    modified=' '.join(emoji.get_emoji_regexp().split(tweet))\r\n",
    "    modified=modified.split()\r\n",
    "    for i,emoji1 in enumerate(modified):\r\n",
    "        if emoji1 in UNICODE_EMOJI['pt']:\r\n",
    "            modified[i]=UNICODE_EMOJI['pt'][emoji1].replace(':','')\r\n",
    "        else:\r\n",
    "            continue\r\n",
    "    modified=' '.join(modified)\r\n",
    "        \r\n",
    "    return modified"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "source": [
    "def limpatudo(text):\r\n",
    "    tira_pontuacao = cleanup(text)\r\n",
    "    tudo_minusculo = minusculo(tira_pontuacao)\r\n",
    "    limpo = (separa_emoji(tudo_minusculo))\r\n",
    "    \r\n",
    "    return limpo\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "source": [
    "def lista(df):\r\n",
    "    listaf = []\r\n",
    "    '''\r\n",
    "    p = ''\r\n",
    "    \r\n",
    "    for i in df:\r\n",
    "        p+=i + ' '\r\n",
    "        '''\r\n",
    "    lista = df.values.tolist() \r\n",
    "    for i in lista:\r\n",
    "        for palavra in i.split():\r\n",
    "            listaf.append(palavra)\r\n",
    "        \r\n",
    "    return listaf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "source": [
    "#Transformando palavras em variáveis categóricas:\r\n",
    "dados['Treinamento'] = dados['Treinamento'].astype('category')\r\n",
    "test['Teste'] = test['Teste'].astype('category')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "source": [
    "dados['Treinamento']=dados['Treinamento'].apply(limpatudo)\r\n",
    "test['Teste']=test['Teste'].apply(limpatudo)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "source": [
    "ir = dados['relevancia'] == 0\r\n",
    "r = dados['relevancia'] == 1\r\n",
    "\r\n",
    "dados_r = dados.loc[r,:]\r\n",
    "dados_ir = dados.loc[ir,:]\r\n",
    "\r\n",
    "dados_r"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>relevancia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to feliz q da p prestar atenção e gostar de vá...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>esquadrão suicida 2 é muito bom amei o filme</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>henriquenarizz kkkkkk amigo mas pior que o esq...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ruanfalco dccomics tô p assistir esquadrão sui...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ei gostei de esquadrão suicida 2 e é só minha ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>achei esquadrão suicida 2 chatokkkk</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>fui assistir o esquadrão suicida 2 muito gore ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>nossa depende hq's dc e tvcinema a marvel poré...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>vi esquadrão suicida 2 hj com minhas irmã e é ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>nossa esquadrao suicida 2 eh tao bom q to me s...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Treinamento  relevancia\n",
       "0    to feliz q da p prestar atenção e gostar de vá...         1.0\n",
       "4         esquadrão suicida 2 é muito bom amei o filme         1.0\n",
       "5    henriquenarizz kkkkkk amigo mas pior que o esq...         1.0\n",
       "8    ruanfalco dccomics tô p assistir esquadrão sui...         1.0\n",
       "16   ei gostei de esquadrão suicida 2 e é só minha ...         1.0\n",
       "..                                                 ...         ...\n",
       "290                achei esquadrão suicida 2 chatokkkk         1.0\n",
       "291  fui assistir o esquadrão suicida 2 muito gore ...         1.0\n",
       "294  nossa depende hq's dc e tvcinema a marvel poré...         1.0\n",
       "296  vi esquadrão suicida 2 hj com minhas irmã e é ...         1.0\n",
       "299  nossa esquadrao suicida 2 eh tao bom q to me s...         1.0\n",
       "\n",
       "[137 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 227
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "source": [
    "ir = test['relevancia'] == 0\r\n",
    "r = test['relevancia'] == 1\r\n",
    "\r\n",
    "test_r = test.loc[r,:]\r\n",
    "test_ir = test.loc[ir,:]\r\n",
    "\r\n",
    "test_ir"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teste</th>\n",
       "      <th>relevancia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>assisti esquadrão suicida 2 com um pouco de at...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vou ver esquadrão suicida 2 hj vcs já viram</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lsdcomixxx adm hj eu sonhei que ele tava no es...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>homemaranha em o esquadrão suicida 2 confirmad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>queria jogat lol mas quero ver esquadrão suici...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>atenção acabei de assistir esquadrão suicida 2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>aquela cena em esquadrão suicida 2 que a harle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>esquadrão suicida 2 parece um filme de zack sn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>esquadrão suicida 2 vai entrar na hbo mds eu o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>vantjens esquadrão suicida 2 um personagem q e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Teste  relevancia\n",
       "0    assisti esquadrão suicida 2 com um pouco de at...           0\n",
       "1          vou ver esquadrão suicida 2 hj vcs já viram           0\n",
       "2    lsdcomixxx adm hj eu sonhei que ele tava no es...           0\n",
       "5    homemaranha em o esquadrão suicida 2 confirmad...           0\n",
       "6    queria jogat lol mas quero ver esquadrão suici...           0\n",
       "..                                                 ...         ...\n",
       "189  atenção acabei de assistir esquadrão suicida 2...           0\n",
       "190  aquela cena em esquadrão suicida 2 que a harle...           0\n",
       "191  esquadrão suicida 2 parece um filme de zack sn...           0\n",
       "192  esquadrão suicida 2 vai entrar na hbo mds eu o...           0\n",
       "193  vantjens esquadrão suicida 2 um personagem q e...           0\n",
       "\n",
       "[115 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 228
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "source": [
    "\r\n",
    "pdr = lista(dados_r.Treinamento)\r\n",
    "\r\n",
    "pdir = lista(dados_ir.Treinamento)\r\n",
    "\r\n",
    "ptr = lista(test_r.Teste)\r\n",
    "\r\n",
    "ptir = lista(test_ir.Teste)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "ptrc = pd.Series(ptr) \r\n",
    "ptirc = pd.Series(ptir) \r\n",
    "pdrc = pd.Series(pdr) \r\n",
    "pdirc = pd.Series(pdir) \r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "source": [
    "fpdr=pdrc.value_counts(True)\r\n",
    "fpdir=pdirc.value_counts(True)\r\n",
    "fptr=ptrc.value_counts(True)\r\n",
    "fptir=ptirc.value_counts(True)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "source": [
    "listatr = ptrc.tolist()\r\n",
    "listatir = ptirc.tolist()\r\n",
    "listadr = pdrc.tolist()\r\n",
    "listadir = pdirc.tolist()\r\n",
    "lista_r = listatr + listadr\r\n",
    "lista_ir = listatir + listadir\r\n",
    "lista_total2 = pd.Series(lista_r + lista_ir)\r\n",
    "\r\n",
    "lista_total1 = pd.Series(listadr+listadir)\r\n",
    "\r\n",
    "Banco_de_dados=set(listadr+listadir)\r\n",
    "\r\n",
    "\r\n",
    "lista_total1.value_counts()\r\n",
    "\r\n",
    "print(Banco_de_dados)\r\n",
    "for i in Banco_de_dados:\r\n",
    "    if i == 'haterdogui':\r\n",
    "        print('HETAR')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'quando', 'mó', 'numa', 'rato', 'infestada', 'rever', 'maravilhosa', 'tubarão', 'pena', 'retorno', 'quanto', 'conferir', 'crédito', 'top10', 'sangrento', 'creditos', 'reaparição', 'hbomaxbr', 'milllllll', 'mal', 'mto', 'ano', 'favsmcavoy', '9nem', 'aberto', 'sua', 'abordaremos', 'arlequina', 'assitiu', 'editado', 'iriam', 'elemento', 'pirocona', 'sala', '️spoiler', 'guy', 'nesse', \"hq's\", 'bolinha', 'httpstcosalo5vig3z', 'mil', 'prestar', 'mané', 'pessoa', 'gabrielnerdland', 'porra', 'dono', 'dccomics', 'óbvio', 'alguma', 'doninha', 'pqp', 'falar', 'tambem', 'primeiro', '3', 'carai', 'ratcatcher', 'páginas', 'ou', 'reassistir', 'ao', 'craig', 'cara', 'assassinos+', 'blackpink', 'nada', 'como', 'no', 'coelho', 'selecionou', 'arqueiro', '3boss', 'final', 'tenso', 'simplesmente', 'as', 'gabixferrars', 'internet', 'sei', 'noção', 'referência', 'eu', 'obrigação', 'tss', 'patrulha', 'kkkkkkkkk', 'chega', 'história', 'gloria', 'foda', 'nome', 'baixa', 'gore', 'pediu', 'edit', 'oesquadrãosuicida', 'kkkkkk', 'httpstco8os9nhho9e', 'própria', 'rosto_ruborizado', 'foi', 'cinefila', 'algum', 'mubibrasil', 'fato', 'pós', 'passado', 'mês', 'sim', 'cortar', 'interessar', 'caça', 'vadias', 'ótimo', 'qualidade', 'hj', 'duvido', 'entregou', 'escrever', 'detetive_mulher_pele_morena', 'altas', 'coringa', 'canina', 'tudo', 'httpstcotpjc90fxwq', 'horrível', 'seria', 'nordeste', 'boss', 'isso', 'clime', 'palha', '10era', '“o', 'grandes', 'será', 'diretor', 'aquelas', 'ent', 'coisa', 'entender', 'httpstcotbktnxjmdx', 'aves', 'cenas', 'nunca', 'alguns', 'httpstcogxptz821ue', 'continuam', 'filmes', 'saindo', 'r2206', 'oxentepipoca', 'música', 'parece', 'quero', 'iaewk', 'teline', 'maluco', 'marvel', 'primeiros', 'del', 'noia', 'cabeça', 'o', 'particularmente', '710', 'gomez', 'trevas', 'a', 'altura', 'deveria', 'sinal', 'forte', 'mtt', '160', 'comedia', 'ajudar', 'né', 'ksksksksksks', 'piadinha', 'theoncediaries', 'ramo', '90m€', 'imaginar', 'fazer', 'izabelardg', 'ter+', 'envelopes', 'cadelinha', '007', 'aventureira', '140', 'httpstcoxq3e9z00js', 'httpstcox5csi9xixe', 'violentcrims', 'nuvem', 'malditas', 'perdi', 'acompanhado', 'bommm', 'pipocavegana', 'httpstcounpzka0x8t', 'tempos', 'esperando', 'julgue', 'depois', 'forma', 'gnt', 'candyman', 'tipo', '80', 'furiosos', 'ayer', 'caminhos', 'programa', 'pá', 'estará', 'junto', 'esse', 'empadinhas', 'em', 'vision', 'jack', 'roteirista', 'duduzera25', 'kkkkkkkkkkkkkkkkkkk', 'estou', 'bem', 'algumas', 'semanas', 'amoooooo', 'mt', 'conseguiria', 'mínimo', 'explicação', 'tristes', 'cansativa', 'assisti', 'refletindo', 'porém', 'lana', '9', 'vazio', 'venom', 'vendo', 'cadeladaharley', 'leptospirose', 'wual', 'kkkkkkkkkkkkkkkkkkkk', 'aqui', 'véi', 'franquia', 'gigante', 'aço', 'cinema', 'eli', 'harrypotter', 'longe', 'lançar', 'livro_aberto', 'ate', 'dia', 'conseguir', 'httpstcox3iolxrmiw', 'caçaratos', 'hein', 'jungle', 'cartaz', 'quinn', 'balangar', 'httpstcobc50acicdw', 'ratos', 'ali', 'caí', 'tao', 'santos', 'pirata', 'depressao', 'onde', 'furios', 'repente', 'aquela', 'os', 'seja', '5candyman', '35★', 'chata', 'combinar', 'max', 'escuto', 'dora', 'delírio', 'descobri', 'cmg', 'último', 'mr_antoniojr', 'péssimo', 'tal', 'demais', 'mais', 'amanhã', 'dot', 'terminando', 'sentindo', 'ultimamente', 'poderosa', 'grande', 'fala', 'só', 'rosto_chorando_de_rir', 'top', 'comida', 'logan', 'breve', 'venham', 'tava', 'ninguém', 'pior', 'chefinho', 'olha', 'casa', 'momentos', 'pueblakleydson', 'liarodriguex', 'esmaga', 'arruinando', 'volumes', 'segundo', 'emocionadaaaa', 'gilson', 'boca', 'seguidas', 'maior', 'de', 'beleza', 'fico', 'batman', 'james', 'umas', 'aguardo', 'bomm', 'sexualidade', 'propositalmente', 'usou', 'entendo', 'carinhosamente', 'livros', 'carrinho_de_compras', 'pipoca', 'riot', '1°', 'bomba', 'cartoons', 'fui', 'duas', 'dilma', 'dnv', 'the', 'pensando', 'menos', 'and', 'ep', 'hypados', 'quarta', '1', 'podre', 'sebastian', 'jam', 'deadpool', 'aparece', 'fechamos', 'auge', 'coração_crescendo', 'henriquenarizz', 'merece', 'pré', 'estavam', 'httpstco1habg69eye', 'puto', 'tocar', 'primeira', 'luafengarie', 'guerreira', 'inmydrew', 'q', 'sozinha', 'gracinha', 'jonhcena', 'rei', 'tive', 'ruim', 'rosto_dormindo', 'building', 'pq', 'sonhar', 'bilheteria', 'boas', 'mds', 'filme', 'mesmo', 'lembrar', 'demorei', 'serio', 'metade', '1boss', 'assistam', 'interagir', 'nanue', 'abnerkrill', 'brasília', 'dei', 'tomate', 'estranho', 'pois', 'dinheiro', 'httpstcopecbg6t7aj', 'tão', 'essa', 'florzinha', 'prestando', 'aranha', 'recentemente', 'amando', 'puta', 'ri', 'fosse', 'mortais', 'esperava', 'izzyunou', 'jonh', 'poderia', 'pisou', 'estreia', 'valeu', 'picsart', 'suicida2', 'todos', 'everybodyhateschris', 'achar', 'dublê', 'httpstcovnpavh1elh', 'futuro', 'entrega', 'sexta', 'kinh__', 'isso¿¿¿‽‽', '4esquadrão', 'polkadotman', 'criticar', 'palavreado', 'vcs', 'httpstcofntxc7xjme', 'trilha', 'velozes', 'mts', '️', 'difícil', 'jumanji', 'colocar', 'irma', 'odio', 'maneiro', 'domingo', 'demorado', '📽', 'talvez', 'carvalho', 'pareço', '7candyman', 'diz', 'leticya', 'crítica', 'httpstcod1npcwm4uf', 'inteiro', 'ter', 'gravaram', 'expectativa', 'sos', 'revendo', 'três', 'que', 'melhor', 'comprovante', 'lembro', 'funcionará', 'são', 'frango', 'meio', 'pedro', 'linda', 'katana', 'polka', 'sido', 'spinoff', 'opiniões', 'fa', 'chocado', '2free', 'hq', 'cueca', 'anos', '60', 'nota_de_dólar', 'morre', 'cornetapalestr', 'estrela', 'esquadraosuicida', 'dublado', 'documentário', 'gosto', 'principal', 'heroína', 'blackpinkvotees', 'httpstcofuf60dflx4', 'queria', 'berna', 'vc', 'premiosmtvmiaw', 'pouco', 'guardiões', 'seriamente', 'tentando', '⚠', 'ruins', 'httpstcolgnjorkwxo', 'httpstcojwhnlt6fux', 'motivo', 'muito', 'esquadaro', 'acaso', 'salas', 'obra', '230', 'então', 'chorei', 'parabéns', 'superar', 'action', '90', 'mymaddiecade', 'fora', 'dúvida', 'problemas', 'indo', 'rosto_fazendo_sinal_de_silêncio', 'ruindade', 'parecendo', 'começou', 'diverti', '2……', 'harley', 'unico', 'kkkkkkk', 'sie', 'espécie', 'dublada', 'marcado', 'assistiu', 'finalsem', 'odiooo', 'após', 'mn', 'httpstcoe99lfeue8u', 'constatar', 'porrada', 'margot', 'violência', '136', 'alguém', 'favorito', '4', 'super', 'starro', 'moral', 'olhei', 'jogar', 'do+', 'espero', 'kamila', 'comédia', 'brutal', 'comprei', 'rosto_de_cabeça_para_baixo', 'focar', 'fiquei', 'chamo', 'top10movies', 'ben', 'semana', 'sétima', 'apaixonado', 'assiste', 'novos', 'meu', 'caralhoo', 'seguidos', 'depende', 'arte', 'games', 'enfim', 'fodase', 'trinca', 'produtora', 'rumor', 'tô', 'absurdo', \"6d'artacão\", 'carregou', 'kkkkkkkkkkkkkkkk', 'legal', 'gostar', 'descobrir', 'perfeito', 'oscar', '15', 'buzina', 'dizer', 'tendo', 'suicidesquad2', 'gasto', 'amigo', '8', 'dois', 'querendo', 'httpstcoaxxvi8yagu', 'branca', 'gabopantaleao', 'mundialmente', 'give', 'rindo', 'peacemaker', 'apenas', 'mão_em_v_de_vitória', 'vai', 'pelo', 'dez', 'coisas', 'mariah', 'oq', 'deu', 'américa', 'única', 'thr', 'apareceu', 'preferi', 'aquaman', 'faz', 'idade', 'vi', 'quem', 'david', 'chato', 'iria', 'devem', 'selena', 'httpstconyxuncm39r', 'gostei', 'veremos', 'minha', 'brasileira', 'para', 'americano', 'interpretado', 'alerquina', 'day', 'inesperada', 'pago', 'dias', 'victoria', 'brabo', 'chave', 'morrer', '•', 'assisto', 'massacrou', 'manooooo', 'verde', 'vezes', 'bicho', 'foram', 'unicamente', '29', 'da', 'projeto', 'fim', 'legais', 'apagando', 'rey', 'fotos', 'apelidei', 'ajuda', 'convencional', 'recente', 'lugar', 'impossível', '10velocidade', 'httpstcozhzfbgpjup', 'tela', 'c', 'dó', 'viúva', 'e', 'rosto_com_olho_piscando', 'monte', 'film', 'burrice', 'bi', 'fica', 'gastei', 'imagem', 'thesuicidesquad', 'spoiler', 'arlequinha', 'morrido', 'observações', 'fiz', 'olhando', 'legado', 'estão', 'tentativa', 'triste', 'meia', 'sobre', 'crime', 'httpstcohjlqydin9q', 'visison', 'serem', 'irá', '1shangchi', 'knd', 'por', 'pirataria', 'matando', 'ai', 'estrelada', 'kkkk', '25set', 'glória', 'crl', 'matths_az', 'chinês', 'primeiro…', 'também', 'falou', 'uma', 'nessa', 'hora', 'vez', 'ei', 'graça', 'completa', 'mão_em_v_de_vitória_pele_clara', 'humano', 'apesar', 'rapina', 'falando', 'walfritsch', 'português', 'tarde', 'putt', 'tiro', 'várias', 'completamente', 'sequência', 'trash', 'httpstcoqlmfpewnu5', 'savitargod333', 'liga', 'bizarro', 'mandar', 'piadas', 'sério', 'nossaaaa', 'estilo', 'romulomiranda_', 'leão', 'achei', 'causa', 'pdrbnt', 'loco', 'protegida', 'terminei', 'horrivel', 'científica', 'preferia', 'conká', 'versão', 'real', 'memória', 'ow', 'genero', 'indicou', 'time', 'fundo', 'tooth', 'sem', 'esperar', 'tenho', 'caralhos', 'httpstcouayv6dyjbq', 'posso', 'teve', 'chegam', 'silencioso', 'pegando', 'tarantino', 'cruise', 'decepcionado', '5free', '_leazy', 'exibidos', 'link', 'pessoal', 'mortal', 'alta_tensão', '2016', '3patrulha', '2°', 'gloriagroove', 'promoção', 'httpstcogciymn5lzl', 'achando', 'hbomax', 'seus', 'vou', '17478383', 'idiota', 'atenção', 'prometeu', '9chal', 'era', 'marola', '♡', 'camiseta', '1pedrokkkkk', 'distraída', 'das', 'justamente', 'pro', 'enredo', 'conpletamente', 'httpstcoou0u5lra7w', 'porque', 'bar', 'pacificador', 'shang', 'sou', 'now', 'celestvics', 'tanto', 'rainhas', 'saco', '2021', 'agitado', 'aneis', 'httpstcoxtjfiizobd', '164', 'incendiou', 'finalmente', 'negra', 'falaremos', 'evoluída', 'furiosa', 'tl', 'dubla', 'dele', 'cheguei', 'homem', 'sera', 'qual', 'ela', 'pre', 'ultimato', '01', 'mortes', 'criado', 'n', 'latina', 'httpstcotylvony0qa', 'httpstcogemtnxbeka', 'infiltrado', 'faltando', 'rosto_expressando_desagrado', 'tik', 'jogando', 'conta', 'maravilhoso', 'prazeres', 'termos', 'snyder', 'notimetodie', 'crianças', 'fez', 'frleticya', 'eternos', 'anima', 'chatokkkk', 'pete', 'passarinhos', 'shangchi', 'família', 'free', 'zeldinha', 'têm', 'adudmaia', 'adolescente', 'daniel', 'toca', 'você', 'num', 'ô', 'homi', 'player', 'santosfc', 'bosta', 'hora…', 'quinta', 'cor', 'opinião', 'este', 'john', 'nn', 'meter', 'heroi', '4patrulha', 'sucesso', 'rosto_implorando', 'vitorsfc88', 'pela', 've', 'sempre', 'horrores', 'ainda', 'série', 'salva', 'vingadores', 'te', 'prettylittleliars', 'resumo', 'apaixonada', 'vale', 'kashruno', 'assistir', 'httpstcoyltb4wlzmj', 'ia', 'parte', 'comento', 'mãos_juntas_pele_clara', 'mulher', 'fantástico', 'minds', 'geral', 'problema', 'formado', 'chi', 'nova', 'maratonar', 'bebido', 'et', 'rosto_sorridente_com_óculos_escuros', 'dessa', 'tu', '500', '8mistério', 'sainttropez', 'dos', 'httpstcolk5ktnokoa', 'começo', 'braga', 'na', 'lago', 'after', 'pernambucana', 'dcfandome', 'entre', 'morte', 'imperialismo', 'representatividade', '7', 'podcast', 'estava', 'promo', 'd', 'apx', 'adorei', 'carey', 'puder', 'ficou', 'mano', 'saiba', 'httpstcoy85mi6eyue', 'juliette', 'drive', 'nem', 'violencia', 'claro', 'baby', 'aleluia', 'gabo', 'to', 'mé', 'esquadrao', 'httpstcoqvlchkpkbd', 'arrecadou', 'tá', 'falta', 'toda', 'vocês', 'tvcinema', 'tequila', '❤', 'realmente', 'ganhou', 'vida', 'tempo', '50', 'ponto', 'kct', 'v4', 'fantasy', 'alice', 'fd', 'pensei', 'me', 'wanda', 'jeito', '7a', 'meses', 'herói', 'httpstcolstrxvtagh', 'kageycat', 'p', 'httpstco43yw2yaxfk', 'amazon', 'humanos', 'aniversário', 'bobão', 'vei', 'rosto_sorridente_com_3_corações', 'mes', 'lady', 'milhões', 'terem', 'está', 'httpstcoyjrzd4g2vk', 'celular', 'justiça', 'começa', 'surrou', 'mtvmiawfandomblink', 'gostinho', 'tinha', 'outro', 'ir', 'httpstcoggii9gntcv', 'slk', '2698', 'dceu', 'recentes', 'entrar', '5', 'off', 'gosta', 'assustade', 'ratinhos', 'live', 'consegue', 'colho', 'afirmou', 'matar', 'evagelion', 'mas', 'salvar', 'ganha', 'trata', 'sabia', 'txtpenis', 'meme', 'tml', 'viram', 'suicida', 'cine', 'hater', 'passaram', 'rosto_sorridente_com_olhos_de_coração', 'colapso', 'min', 'rosto_chorando_aos_berros', 'nanathhs', 'httpstcodvu9ldva27', 'glr', 'sorreio', 'at', 'quadrinhos', '16', 'sonora', 'saboroso', '74m€', 'ilhasolteira', 'shanguinho', 'desde', 'gaga', 'avataralendadeaang', 'deve', 'infinita', 'expectativas', 'povo', 'necessito', 'dirigiu', 'httpstcoi2i7gsswhp', 'kkkķkkk', 'pega', 'in', 'ihugkugisaki', 'nao', 'pprt', 'espelho', 'superou', 'junção', 'ver', 'com', 'violadavis', \"8d'artacão\", 'homem_decepcionado_pele_morena_escura', 'samurai', 'esses', 'lá', 'nos', 'inflitrado', 'insiste', 'mesma', 'it', '44', 'httpstcocxyvxct1de', 'juntou', 'ok', 'quarto', 'importa', 'nham', 'deizao', 'perde', 'suicidesquad', 'sendo', 'supera', 'anéis', 'pai', 'robbie', 'szkntos', 'porcaria', 'restaurante', 'legendado', 'incrível', 'boomerang', 'fogo', 'ser', 'httpstcoqdcgrtdyfp', 'preguiça', 'heyalviverde', 'bons', 'absolutamente', 'não', 'rir', 'sessão', 'bizarras', 'saiu', 'httpstcoecf8fzqmpn', 'pareça', 'murders', '21', 'httpstcosgvodey2pe', 'ruanfalco', 'horas', 'pé', 'muuuuito', 'dentro', 'agr', 'kombat', 'exatamente', 'setembro', 'dívida', 'solto', 'sai', 'k', 'caramba', 'quão', 'merecia', 'kskwkdkwjdsk', 'encheu', 'fofo', 'zzz', 'chorar', 'ta', 'man', 'sorry', 'tirando', 'pariu', 'força', 'google', 'tocando', 'pra', 'ggukchase', 'merda', 'ó', 'ganhamos', '2015', 'tiram', 'issues', 'dc', 'news', 'é', 'br', 'nenhum', 'estamos', 'próximo', 'nerdboomer', 'cleo', 'wooyngoth', 'httpstcoljgtzwynrp', 'pata', 'hoje', 'davidbqwie', 'lenda', 'passou', 'hbo', 'quer', 'ganhei', 'acontecer', 'aí', 'do', 'remake', 'timeline', 'ew', 'shot', 'vera', 'worst', 'kkk', 'sair', 'lançamento', 'vo', 'mera', 'davidson', 'nooo', 'hbomaxbr_portal', 'daddy', 'kimbeerly_02', 'eh', 'partes', 'mundo', 'shark', 'minutos', 'nanaue', '2after', 'desperdiçaram', 'minhas', 'httpstco5wv9gje0it', 'pos', 'dmsssssss', 'até', 'ilhadenotícias', 'ingresso', 'bom', 'groove', 'dar', 'sinceramente', '12', 'um', 'produzindo', 'honestamente', 'httpstcopwsse0q2p8', 'feliz', 'coletivo', 'sweet', 'chocada', 'ca', 'chorando', 'piores', 'httpstcoyychedtc1f', 'rolando_no_chão_de_rir', '61m€', 'vote', 'lançou', 'queen', 'parar', 'ne3djimi', 'marcusrocha', 'acho', 'respires', 'desenhos', '6', 'desse', 'sangue', 'aquilo', 'vários', 'httpstcogjrnqdqctp', 'lança', 'mim', 'haterdogui', 'superior', 'cidade', 'aquele', 'verdadeira', 'poderoso', 'sabe', 'amei', 'assim', 'antes', 'pedrocertezas', 'poder', 'helena', 'passada', 'descendo', 'httpstcoeuxdvthonm', 'nas', 'atriz', 'solo', 'doin', 'vermelho', 'sobrinha', 'passando', 'kkkkk', 'suicída', 'stallone', 'ajude', 'precisamos', 'fantásticas', 'lançados', 'anuncio', 'atoa', 'faria', '10', '70', 'background', 'lilmarks1', 'ja', 'personagens', 'criminal', 'ontem', '75', 'otimo', 'fofos', 'convicta', 'karol', '“a', 'daniel123desa', 'se', 'vol', 'ah', 'cena', 'httpstco8focn5w6rc', 'divertido', 'digo_horacio', 'olhos', 'i', 'possível', 'fraquíssimo', 'novo', 'encantado', 'lixão', 'desnecessárias', 'favorita', 'passar', 'tok', 'torta', 'esmurra', 'episodio', 'juliettefinfos', 'mkkkkkkkkmk', 'deus', 'httpstcozt5k6osvta', 'todomundoodeiaochris', 'dedo', 'escolher', 'desencontro', 'maneirona', 'assistindo', 'agora', 'alvo', 'noite', '55', 'classificativa', 'daqui', '98m€', 'kkkkkkkkkk', 'ótima', '41m€', 'escolheu', 'viu', 'vamos', 'membros', 'nele', 'negócios', 'galáxia', 'gunn', 'seres', 'recomendo', 'significa', 'melhores', 'tem', 'series', '2', 'capitão', 'certeza', '2017', 'pessoas', 'qualquer', 'littlecansada', 'shazam', 'acabei', 'rosto_nauseado', 'prr', 'já', 'uns', 'erros', 'nanaui', 'esquadrão', '+16', 'amo', 'podia', 'ele', 'recémchegados', 'voz', 'gente', 'ator', 'comum', 'estiver', 'personagem', 'imperfeições', 'muuuito', 'space', 'harrymetalfan', 'nota', 'only', 'gun', 'irmã', 'aclamada', 'ficção', 'us', 'feedbacks', 'gduvivier', 'mtvmiawhitlovesickgirls', 'últimos', 'tbm', 'eles', 'vilão', 'fase', 'vente', 'conseguiu', 'httpstcoitczgcjub0', 'dela', 'data', 'é……cara', 'moscãoteiros', 'milhão', 'vsf', 'bastante', 'nossa', 'crush', '“vilões', 'costaaaaaaas', 'yasutoky', '2tava', 'todo', '6esquadrão', 'eram', 'outra', 'louca', 'paulada', 'provavelmente', 'guerra'}\n",
      "HETAR\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "source": [
    "# probabilidade de ser relevante:\r\n",
    "pr = len(listadr)/len(lista_total1)\r\n",
    "# probabilidade de ser irrelevante:\r\n",
    "pi = len(listadir)/len(lista_total1)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "print(pr+pi)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "source": [
    "def prob_R_I(text):\r\n",
    "    Prob1 = 1\r\n",
    "    Prob2 = 1\r\n",
    "    for i in text.split():\r\n",
    "        fr = fpdr[i]\r\n",
    "        fir = fpdir[i]\r\n",
    "        Alaplace_relevante = (fr+1)/(len(listadr)+len(Banco_de_dados))\r\n",
    "        Alaplace_irrelevante = (fir+1)/(len(listadir)+len(Banco_de_dados))\r\n",
    "        Prob1 *= Alaplace_relevante\r\n",
    "        Prob2 *= Alaplace_irrelevante\r\n",
    "    if Prob1*pr > Prob2*pi:\r\n",
    "        return 1\r\n",
    "    else: \r\n",
    "        return 0 \r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "source": [
    "print(prob_R_I('marola'))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'marola'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2894\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2895\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'marola'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-237-68398088bfb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob_R_I\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'marola'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-233-e94640e6b4d7>\u001b[0m in \u001b[0;36mprob_R_I\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mProb2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mfr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfpdr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mfir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfpdir\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mAlaplace_relevante\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfr\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlistadr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBanco_de_dados\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 882\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[1;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 989\u001b[1;33m         \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    990\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2895\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'marola'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "source": [
    "def freq_absoluta(palavra,freq_absoluta_conjunto):\r\n",
    "    \"\"\"\r\n",
    "    Conta quantas vezes determinada palavra apareceu \r\n",
    "    na respectiva categoria, seja relevante ou irrelevante\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    try:\r\n",
    "        return freq_absoluta_conjunto[palavra]\r\n",
    "    \r\n",
    "    except:\r\n",
    "        return 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "source": [
    "frase = 'esquadrao suicida é muito bom'\r\n",
    "n = 1\r\n",
    "for i in fpdir[frase.split()]:\r\n",
    "    n *= i \r\n",
    "Probpdir = n   \r\n",
    "print(Probpdir)\r\n",
    "\r\n",
    "n = 1\r\n",
    "for i in fpdr[frase.split()]:\r\n",
    "    n *= i \r\n",
    "Probpdr = n   \r\n",
    "print(Probpdr)\r\n",
    "\r\n",
    "n = 1\r\n",
    "for i in fptr[frase.split()]:\r\n",
    "    n *= i \r\n",
    "Probfptr = n   \r\n",
    "print(Probfptr)\r\n",
    "\r\n",
    "n = 1\r\n",
    "for i in fptir[frase.split()]:\r\n",
    "    n *= i\r\n",
    "Probfptir = n   \r\n",
    "print(Probfptir)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.3847596269482953e-12\n",
      "1.8288089274852862e-09\n",
      "2.159033147307012e-09\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "\"Passing list-likes to .loc or [] with any missing labels is no longer supported. The following labels were missing: Index(['bom'], dtype='object'). See https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\"",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-236-e8281b561402>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfptir\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfrase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mn\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mProbfptir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    904\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 906\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    907\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_with\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m         \u001b[1;31m# handle the dup indexing case GH#4246\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_values_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    877\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 879\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1097\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot index with multidimensional key\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m             \u001b[1;31m# nested tuple slicing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m         \u001b[1;31m# A collection of keys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m         \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m         return self.obj._reindex_with_indexers(\n\u001b[0;32m   1039\u001b[0m             \u001b[1;33m{\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1255\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0moption_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"display.max_seq_items\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"display.width\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1315\u001b[1;33m                     raise KeyError(\n\u001b[0m\u001b[0;32m   1316\u001b[0m                         \u001b[1;34m\"Passing list-likes to .loc or [] with any missing labels \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1317\u001b[0m                         \u001b[1;34m\"is no longer supported. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Passing list-likes to .loc or [] with any missing labels is no longer supported. The following labels were missing: Index(['bom'], dtype='object'). See https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\""
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "### Verificando a performance do Classificador\n",
    "\n",
    "Agora você deve testar o seu classificador com a base de Testes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "### Concluindo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "### Qualidade do Classificador a partir de novas separações dos tweets entre Treinamento e Teste\n",
    "\n",
    "Caso for fazer esse item do Projeto"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "## Aperfeiçoamento:\n",
    "\n",
    "Trabalhos que conseguirem pelo menos conceito B vão evoluir em conceito dependendo da quantidade de itens avançados:\n",
    "\n",
    "* IMPLEMENTOU outras limpezas e transformações que não afetem a qualidade da informação contida nos tweets. Ex: stemming, lemmatization, stopwords\n",
    "* CORRIGIU separação de espaços entre palavras e emojis ou entre emojis e emojis\n",
    "* CRIOU categorias intermediárias de relevância baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante. Pelo menos quatro categorias, com adição de mais tweets na base, conforme enunciado. (OBRIGATÓRIO PARA TRIOS, sem contar como item avançado)\n",
    "* EXPLICOU porquê não pode usar o próprio classificador para gerar mais amostras de treinamento\n",
    "* PROPÔS diferentes cenários para Naïve Bayes fora do contexto do projeto\n",
    "* SUGERIU e EXPLICOU melhorias reais com indicações concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "* FEZ o item 6. Qualidade do Classificador a partir de novas separações dos tweets entre Treinamento e Teste descrito no enunciado do projeto (OBRIGATÓRIO para conceitos A ou A+)"
   ],
   "metadata": {},
   "attachments": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "## Referências"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "978d606978bb26dc1196d980de5f982f9f8ebb0565a7cd0735526ac935a81f1b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}