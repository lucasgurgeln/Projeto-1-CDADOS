{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Projeto 1 - Ci√™ncia dos Dados"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nome: Jo√£o Pedro Reis Lima\r\n",
    "\r\n",
    "Nome: Lucas Gurgel "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Aten√ß√£o: Ser√£o permitidos grupos de tr√™s pessoas, mas com uma rubrica mais exigente. Grupos deste tamanho precisar√£o fazer um question√°rio de avalia√ß√£o de trabalho em equipe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "Carregando algumas bibliotecas:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "%matplotlib inline\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "print('Esperamos trabalhar no diret√≥rio')\r\n",
    "print(os.getcwd())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Esperamos trabalhar no diret√≥rio\n",
      "c:\\Users\\lucas\\Downloads\\Projeto-1-CDADOS\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Carregando a base de dados com os tweets classificados como relevantes e n√£o relevantes:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "filename = 'esquadr√£o suicida 2.xlsx'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "dados = pd.read_excel(filename)\r\n",
    "dados.head(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>relevancia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to feliz q da p prestar aten√ß√£o e gostar de v√°...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>os roteirista de esquadr√£o suicida 2 devem ter...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>assistir esse esquadr√£o suicida 2 de marola</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e eu e gilson que fechamos a sala de cinema p ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>esquadr√£o suicida 2 √© muito bom, amei o filme</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Treinamento  relevancia\n",
       "0  to feliz q da p prestar aten√ß√£o e gostar de v√°...         1.0\n",
       "1  os roteirista de esquadr√£o suicida 2 devem ter...         0.0\n",
       "2        assistir esse esquadr√£o suicida 2 de marola         0.0\n",
       "3  e eu e gilson que fechamos a sala de cinema p ...         0.0\n",
       "4      esquadr√£o suicida 2 √© muito bom, amei o filme         1.0"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "test = pd.read_excel(filename, sheet_name = 'Teste')\r\n",
    "test.head(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teste</th>\n",
       "      <th>relevancia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>assisti esquadr√£o suicida 2 com um pouco de at...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vou ver esquadr√£o suicida 2 hj, vcs j√° viram?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@lsdcomixxx adm hj eu sonhei que ele tava no e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ranking personagens do esquadr√£o suicida:\\n\\n1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mano gloria groove no esquadr√£o suicida 2 fico...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Teste  relevancia\n",
       "0  assisti esquadr√£o suicida 2 com um pouco de at...           0\n",
       "1      vou ver esquadr√£o suicida 2 hj, vcs j√° viram?           0\n",
       "2  @lsdcomixxx adm hj eu sonhei que ele tava no e...           0\n",
       "3  ranking personagens do esquadr√£o suicida:\\n\\n1...           1\n",
       "4  mano gloria groove no esquadr√£o suicida 2 fico...           1"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "## Classificador autom√°tico de sentimento\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "N√≥s consideramos relavante coment√°rios que demostravam alguma critica ao produto tais como: Se a pessoa gostou ou n√£o da obra, o que ela sentiu(emo√ß√£o), tamb√©m consideramos personagens favoritos como algo relavante \r\n",
    "\r\n",
    "N√£o consideramos comentarios de situa√ß√µes adversas sem rela√ß√£o direta com a criticidade da obra, tal como comentarios pessoais"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "### Montando um Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import re \r\n",
    "import emoji\r\n",
    "from emoji import UNICODE_EMOJI\r\n",
    "\r\n",
    "\r\n",
    "def cleanup(text):\r\n",
    "    \"\"\"\r\n",
    "        Fun√ß√£o de limpeza muito simples que troca alguns sinais b√°sicos por espa√ßos\r\n",
    "    \"\"\"\r\n",
    "    #import string\r\n",
    "    punctuation = '[‚Äù@\\-/!.:?;,''\"|()#$%¬®&*]' # Note que os sinais [] s√£o delimitadores de um conjunto.\r\n",
    "    pattern = re.compile(punctuation)\r\n",
    "    text_subbed = re.sub(pattern, '', text)\r\n",
    "    return text_subbed"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def minusculo(text):\r\n",
    "    return text.lower()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def separa_emoji(tweet):\r\n",
    "   \r\n",
    "    modified=' '.join(emoji.get_emoji_regexp().split(tweet))\r\n",
    "    modified=modified.split()\r\n",
    "    for i,emoji1 in enumerate(modified):\r\n",
    "        if emoji1 in UNICODE_EMOJI['pt']:\r\n",
    "            modified[i]=UNICODE_EMOJI['pt'][emoji1].replace(':','')\r\n",
    "        else:\r\n",
    "            continue\r\n",
    "    modified=' '.join(modified)\r\n",
    "        \r\n",
    "    return modified"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def limpatudo(text):\r\n",
    "    tira_pontuacao = cleanup(text)\r\n",
    "    tudo_minusculo = minusculo(tira_pontuacao)\r\n",
    "    limpo = (separa_emoji(tudo_minusculo))\r\n",
    "    \r\n",
    "    return limpo\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def lista(df):\r\n",
    "    listaf = []\r\n",
    "    '''\r\n",
    "    p = ''\r\n",
    "    \r\n",
    "    for i in df:\r\n",
    "        p+=i + ' '\r\n",
    "        '''\r\n",
    "    lista = df.values.tolist() \r\n",
    "    for i in lista:\r\n",
    "        for palavra in i.split():\r\n",
    "            listaf.append(palavra)\r\n",
    "        \r\n",
    "    return listaf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "#Transformando palavras em vari√°veis categ√≥ricas:\r\n",
    "dados['Treinamento'] = dados['Treinamento'].astype('category')\r\n",
    "test['Teste'] = test['Teste'].astype('category')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "dados['Treinamento']=dados['Treinamento'].apply(limpatudo)\r\n",
    "test['Teste']=test['Teste'].apply(limpatudo)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "ir = dados['relevancia'] == 0\r\n",
    "r = dados['relevancia'] == 1\r\n",
    "\r\n",
    "dados_r = dados.loc[r,:]\r\n",
    "dados_ir = dados.loc[ir,:]\r\n",
    "\r\n",
    "dados_r"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>relevancia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to feliz q da p prestar aten√ß√£o e gostar de v√°...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>esquadr√£o suicida 2 √© muito bom amei o filme</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>henriquenarizz kkkkkk amigo mas pior que o esq...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ruanfalco dccomics t√¥ p assistir esquadr√£o sui...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ei gostei de esquadr√£o suicida 2 e √© s√≥ minha ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>achei esquadr√£o suicida 2 chatokkkk</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>fui assistir o esquadr√£o suicida 2 muito gore ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>nossa depende hq's dc e tvcinema a marvel por√©...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>vi esquadr√£o suicida 2 hj com minhas irm√£ e √© ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>nossa esquadrao suicida 2 eh tao bom q to me s...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Treinamento  relevancia\n",
       "0    to feliz q da p prestar aten√ß√£o e gostar de v√°...         1.0\n",
       "4         esquadr√£o suicida 2 √© muito bom amei o filme         1.0\n",
       "5    henriquenarizz kkkkkk amigo mas pior que o esq...         1.0\n",
       "8    ruanfalco dccomics t√¥ p assistir esquadr√£o sui...         1.0\n",
       "16   ei gostei de esquadr√£o suicida 2 e √© s√≥ minha ...         1.0\n",
       "..                                                 ...         ...\n",
       "290                achei esquadr√£o suicida 2 chatokkkk         1.0\n",
       "291  fui assistir o esquadr√£o suicida 2 muito gore ...         1.0\n",
       "294  nossa depende hq's dc e tvcinema a marvel por√©...         1.0\n",
       "296  vi esquadr√£o suicida 2 hj com minhas irm√£ e √© ...         1.0\n",
       "299  nossa esquadrao suicida 2 eh tao bom q to me s...         1.0\n",
       "\n",
       "[137 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "ir = test['relevancia'] == 0\r\n",
    "r = test['relevancia'] == 1\r\n",
    "\r\n",
    "test_r = test.loc[r,:]\r\n",
    "test_ir = test.loc[ir,:]\r\n",
    "\r\n",
    "test_ir"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teste</th>\n",
       "      <th>relevancia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>assisti esquadr√£o suicida 2 com um pouco de at...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vou ver esquadr√£o suicida 2 hj vcs j√° viram</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lsdcomixxx adm hj eu sonhei que ele tava no es...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>homemaranha em o esquadr√£o suicida 2 confirmad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>queria jogat lol mas quero ver esquadr√£o suici...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>aten√ß√£o acabei de assistir esquadr√£o suicida 2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>aquela cena em esquadr√£o suicida 2 que a harle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>esquadr√£o suicida 2 parece um filme de zack sn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>esquadr√£o suicida 2 vai entrar na hbo mds eu o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>vantjens esquadr√£o suicida 2 um personagem q e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Teste  relevancia\n",
       "0    assisti esquadr√£o suicida 2 com um pouco de at...           0\n",
       "1          vou ver esquadr√£o suicida 2 hj vcs j√° viram           0\n",
       "2    lsdcomixxx adm hj eu sonhei que ele tava no es...           0\n",
       "5    homemaranha em o esquadr√£o suicida 2 confirmad...           0\n",
       "6    queria jogat lol mas quero ver esquadr√£o suici...           0\n",
       "..                                                 ...         ...\n",
       "189  aten√ß√£o acabei de assistir esquadr√£o suicida 2...           0\n",
       "190  aquela cena em esquadr√£o suicida 2 que a harle...           0\n",
       "191  esquadr√£o suicida 2 parece um filme de zack sn...           0\n",
       "192  esquadr√£o suicida 2 vai entrar na hbo mds eu o...           0\n",
       "193  vantjens esquadr√£o suicida 2 um personagem q e...           0\n",
       "\n",
       "[115 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "\r\n",
    "pdr = lista(dados_r.Treinamento)\r\n",
    "\r\n",
    "pdir = lista(dados_ir.Treinamento)\r\n",
    "\r\n",
    "ptr = lista(test_r.Teste)\r\n",
    "\r\n",
    "ptir = lista(test_ir.Teste)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "ptrc = pd.Series(ptr) \r\n",
    "ptirc = pd.Series(ptir) \r\n",
    "pdrc = pd.Series(pdr) \r\n",
    "pdirc = pd.Series(pdir) \r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "fpdr=pdrc.value_counts(True)\r\n",
    "fpdir=pdirc.value_counts(True)\r\n",
    "fptr=ptrc.value_counts(True)\r\n",
    "fptir=ptirc.value_counts(True)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "listatr = ptrc.tolist()\r\n",
    "listatir = ptirc.tolist()\r\n",
    "listadr = pdrc.tolist()\r\n",
    "listadir = pdirc.tolist()\r\n",
    "lista_r = listatr + listadr\r\n",
    "lista_ir = listatir + listadir\r\n",
    "lista_total2 = pd.Series(lista_r + lista_ir)\r\n",
    "\r\n",
    "lista_total1 = pd.Series(listadr+listadir)\r\n",
    "\r\n",
    "Banco_de_dados=set(listadr+listadir)\r\n",
    "\r\n",
    "\r\n",
    "lista_total1.value_counts()\r\n",
    "\r\n",
    "print(Banco_de_dados)\r\n",
    "for i in Banco_de_dados:\r\n",
    "    if i == 'joao':\r\n",
    "        print('HETAR')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'dot', 'na', 'httpstcodvu9ldva27', '2698', 'dessa', 'ser√°', 'grandes', 'passando', 'beleza', 'httpstcoggii9gntcv', 'milh√£o', 'harrymetalfan', 'esquadaro', '70', 'rosto_sorridente_com_3_cora√ß√µes', 'ela', 'ao', 'reapari√ß√£o', 'tendo', 'continuam', 'pessoal', 'protegida', 'oscar', 'carrinho_de_compras', 'tarantino', 'homem_decepcionado_pele_morena_escura', 'venham', 'alerquina', 'cadelinha', 'escolheu', 'menos', 'bastante', 'talvez', 'os', 'mosc√£oteiros', '21', 'observa√ß√µes', 'faz', 'hypados', 'm√©', 'fechamos', 'chin√™s', 'httpstco43yw2yaxfk', 'sem', 'saboroso', 't√¥', 'fant√°sticas', 'geral', 'chatokkkk', 'estava', 'p√≥s', 'olhos', 'doninha', 'ggukchase', 'e', 'slk', 'est√£o', 'querendo', 'anos', 'legais', 'divertido', '50', 'httpstcopecbg6t7aj', 'pai', '4esquadr√£o', 'nooo', 'ayer', 'amoooooo', 'min', 'passou', '√∫nica', 'vez', 'no', 'esperava', 'braga', 'minds', 'sentindo', 'ir√°', 'nanaui', 'logan', 'nesse', 'player', 'bosta', 'franquia', 'julgue', 'por', 'her√≥i', 'malditas', 'depende', 'bras√≠lia', 'precisamos', 'suicidesquad', 'tempo', 'amo', 'pouco', 'adorei', 'prometeu', 'vision', 'sua', 'refletindo', 'mtvmiawhitlovesickgirls', 'algum', 'murders', 'ai', 'aqui', 'pensei', 'tem', 'sie', 'nota_de_d√≥lar', 'serio', 'tml', 'tao', 'dele', 'm√≠nimo', 'httpstcofuf60dflx4', 'bob√£o', 'carregou', 'bizarras', 'humano', 'amei', 'novos', 'nao', 'para', 'gosto', 'day', 'davidson', 'justamente', 'le√£o', 'sendo', 'httpstcoeuxdvthonm', 'vamos', 'rosto_ruborizado', 'rosto_dormindo', 'breve', 'quarta', 'everybodyhateschris', 'estar√°', 'viram', 'fase', 'man√©', 'voc√™s', 'chata', 'building', 'como', 'httpstcoxq3e9z00js', 'bomm', 'convicta', 'exibidos', 'violencia', 'branca', 'cr√©dito', 'depressao', 'exatamente', 'mr_antoniojr', 'morrido', 'sobre', 'romulomiranda_', 'jumanji', 'linda', 'httpstcocxyvxct1de', 'escolher', '10velocidade', 'futuro', 'livros', 'httpstcoljgtzwynrp', 'tinha', 'tipo', 'milllllll', 'parte', 'esperar', 'preferi', 'momentos', 'shazam', 'dos', 'victoria', 'shangchi', 'pr√≥pria', 'tocando', 'nn', 'jeito', 'gal√°xia', 'entender', 'lembrar', '‚Ä¢', 'pipocavegana', 'ta', 'demorado', 'demais', 'tocar', 'pr√©', 'nas', 'costaaaaaaas', 'cornetapalestr', 'vitorsfc88', 'chocada', 'puta', 'entendo', '2¬∞', 'este', 'harrypotter', 'rindo', 'ca√≠', 'szkntos', 'apenas', 'sabe', 'blackpink', 'samurai', '√≥tima', 'caramba', 'zzz', 'certeza', 'ter+', 'completamente', 'hbomaxbr_portal', 'atoa', 'httpstcopwsse0q2p8', 'violadavis', 'dela', 'alguns', 'enfim', '29', 'baby', 'cabe√ßa', 'porque', 'carvalho', 'merecia', 'kkkkkkkkkkkkkkkkkkk', 'httpstco8focn5w6rc', 'porcaria', 'por√©m', 'perfeito', 'apx', 'se', 'imperfei√ß√µes', 'ca', '_leazy', 'httpstcoi2i7gsswhp', 'sobrinha', 'gostar', 'ano', 'problema', 'document√°rio', 'mtvmiawfandomblink', 'shanguinho', 'coletivo', 'carey', 'abnerkrill', 'burrice', 'puder', 'man', 'baixa', 'ontem', 'vera', 'veremos', 'onde', '‚Äúo', 'nota', 'apareceu', 'quarto', 'bar', 'tl', 'voc√™', 'sangue', 'shot', 'as', 'imperialismo', 'muito', 'vi', 'maluco', 'convencional', 'pos', 'chi', 'isso', 'entrar', 'pq', 'foda', 'aparece', 'httpstco1habg69eye', 'poderia', 'completa', 'pela', 'nanaue', 'principal', 'incendiou', 'monte', 'latina', 'trash', 'glr', 'ator', '90m‚Ç¨', 'piadinha', 'background', 'parab√©ns', 'httpstconyxuncm39r', 'estiver', 'anuncio', 'sucesso', 'vers√£o', 'estrelada', 'mesmo', 'maneirona', 'br', 'foi', 'dizer', 'horr√≠vel', 'altas', 'meio', 'pre', 'alguma', 'agora', 'comida', 'ca√ßaratos', 'txtpenis', 'httpstcotylvony0qa', 'Ô∏è', 't√™m', 'santos', 'hater', 'link', 'httpstcouayv6dyjbq', 'insiste', 'encantado', 'hbomax', 'feliz', 'pisou', 'comedia', '007', 'gracinha', 'c', 'tive', 'haterdogui', '√≥timo', 'altura', 'estrela', 'seria', 'oq', 'give', 'come√ßou', 'pessoa', 'tenso', 'projeto', 'cmg', 'incr√≠vel', 'pirataria', 'crian√ßas', 'gasto', 'duas', 'louca', 'matar', 'esses', 'buzina', 'fazer', 'apaixonada', 'sido', 'vendo', 'hbo', 'falta', 'ben', 'liga', 'conk√°', 'forma', '74m‚Ç¨', 'httpstcolstrxvtagh', '164', 'kkkkk', 'pago', 'passar', 'achei', 'comprovante', 'com', 'vsf', 'fui', 'falaremos', 'theoncediaries', 'amazon', 'berna', 'vai', 'tudo', 'apesar', 'acompanhado', 'conta', 'sozinha', 'fosse', 'nuvem', 'estamos', 'suicidesquad2', 'vol', 'meter', 'vazio', '9', 'mubibrasil', 'encheu', 'entre', 'pra', 'gastei', 'agr', 'leptospirose', 'm√£o_em_v_de_vit√≥ria', 'cadeladaharley', 'milh√µes', 'seriamente', 'consegue', '‚ù§', \"8d'artac√£o\", 'significa', 'knd', 'eram', 'jogando', 'nada', 'pelo', 'tr√™s', 'tempos', 'promo', 'mkkkkkkkkmk', 'o', 'serem', 'parar', '5', 'sou', 'filmes', 'ruindade', '01', 'seja', 'ruim', '2016', 'roteirista', 'honestamente', 'hero√≠na', 'cansativa', 'dedo', 'selena', 'povo', 'cine', 'superior', 'cidade', 'mto', 'boss', 'nessa', 'v√°rias', 'ir', 'saiba', 'kkkkkk', 'suicida2', 'mim', 'apaixonado', 'produzindo', 'httpstcoitczgcjub0', 'sair', 'tiro', 'fim', 'tbm', '2017', 'respires', 'aberto', 'kamila', 'rever', 'cinefila', 'series', 'indo', 'kkkkkkkkk', 'ajude', 'celular', 'fora', 'rir', 'guerra', 'duvido', '3boss', 'tamb√©m', 'gostei', 'aranha', 'velozes', 'oesquadr√£osuicida', 'seres', 'nele', 'teve', 'falando', 'spinoff', 'aleluia', 'deadpool', 'ent', 'acho', '44', 'd√∫vida', 'uma', 'arte', 'mundo', 'digo_horacio', 'only', 'real', 'abordaremos', 'edit', 'lenda', 'sess√£o', 'p√©ssimo', '√≥', 'deu', 'homem', 'eli', 'dmsssssss', 'viol√™ncia', 'come√ßo', 'nanathhs', 'stallone', '160', 'finalsem', '√∫ltimo', 'tava', 'ainda', 'crl', 'sim', 'fofos', 'manooooo', 'tirando', 'batman', 'tomate', 'inmydrew', 'max', 'minha', 'irm√£', 'ganhei', 'saiu', 'empadinhas', '√©‚Ä¶‚Ä¶cara', 'gente', 'visison', 'fa', 'mesma', 'poder', 'tentativa', 'thr', 'httpstcoxtjfiizobd', 'gostinho', 'ponto', 'httpstcogciymn5lzl', 'tss', 'poss√≠vel', 'paulada', 'massacrou', 'kinh__', 'sabia', 'assim', 'tok', 'poderoso', 'um', 'james', 'arlequina', 'escrever', 'd', 'bons', 'salvar', 'fala', 'valeu', 'picsart', 'issues', 'vo', 'candyman', 'humanos', 'florzinha', 'sainttropez', 'creditos', 'recomendo', 'drive', 'estranho', 'juntou', 'imagem', 'muuuuito', 'mais', 'realmente', 'ali', 'novo', 'ep', 'trevas', 'devem', '16', 'izzyunou', 'faria', 'tequila', 'passaram', 'pirata', 'mil', 'desnecess√°rias', 'meia', 'kimbeerly_02', 'sorreio', 'reassistir', 'hoje', 'assistindo', 'prestando', 'pro', 'legal', 'lan√ßamento', 'space', 'bilheteria', 'aquilo', 'expectativa', 'premiosmtvmiaw', 'motivo', 'auge', 'dilma', 'deveria', 'verdadeira', 'notimetodie', 'anivers√°rio', 'funcionar√°', 'blackpinkvotees', 'caminhos', 'time', 'pedrocertezas', 'sequ√™ncia', 'toca', 'era', 'dar', 'americano', 'crush', 'iria', 'perde', 'favsmcavoy', 'kkkƒ∑kkk', 'kskwkdkwjdsk', 'spoiler', 'furios', 'httpstcoyychedtc1f', 'httpstcoy85mi6eyue', 'jam', '61m‚Ç¨', 'imposs√≠vel', '136', 'polka', 'chorar', 'kkkkkkk', 'kkkkkkkkkkkkkkkkkkkk', 'vote', 'httpstco8os9nhho9e', 'trata', 'em', 'diretor', 'httpstcogxptz821ue', 'tela', 'marcusrocha', 'combinar', '75', 'constatar', 'fogo', 'kkk', 'legendado', 'guardi√µes', 'minhas', '15', '10era', 'httpstcozhzfbgpjup', 'mariah', 'groove', 'cinema', 'nos', 'bebido', 'comprei', 'livro_aberto', 'estilo', 'yasutoky', 'distra√≠da', 'nossa', 'propositalmente', 'adudmaia', 'melhor', 'conferir', 'retorno', 'sonora', 'jun√ß√£o', 'evolu√≠da', 'pipoca', 'conseguiu', 'aquelas', 'fam√≠lia', 'cena', 'formado', 'maior', 'us', 'sebastian', 'sinceramente', 'ou', 'avataralendadeaang', 'n', 'gabixferrars', 'internet', 'bolinha', 'ew', 'lan√ßa', 'marcado', 'nenhum', 'm√≥', 'pariu', 'caralhos', 'sala', 'it', 'mortal', 'olhando', 'indicou', 'desenhos', 'httpstcoecf8fzqmpn', 'ter', '7', 'lan√ßar', 'del', 'fd', 'escuto', 'deve', 'forte', 'quinta', 'est√°', 'm√∫sica', 'num', '‚ô°', 'nanue', 't√°', 'esmaga', 'hist√≥ria', 'assistiu', 'noite', 'todo', 'httpstcoqvlchkpkbd', 'mds', 'todos', '55', 'esp√©cie', 'sonhar', 'pernambucana', 'assitiu', 'qu√£o', 'fez', 'teline', 'rumor', 'sorry', 'httpstcoqlmfpewnu5', 'melhores', 'bommm', 'espelho', 'fico', 'pete', 'httpstcox5csi9xixe', 'obriga√ß√£o', 'an√©is', 'i', 'assassinos+', 'lana', 'vadias', '+16', 'recente', 'guerreira', 'mandar', 'genero', 'fic√ß√£o', 'torta', 'live', 'demorei', 'bomba', 'boca', 'sera', 'semanas', 'absolutamente', 'achar', 'quero', 'personagem', '‚Äúa', 'chegam', '2after', 'iaewk', 'heyalviverde', 'vingadores', 'negra', 'ratinhos', 'httpstcoyltb4wlzmj', 'izabelardg', 'anima', 'after', 'boomerang', 'espero', 'the', 'matando', 'cora√ß√£o_crescendo', 'pegando', 'pdrbnt', 'acontecer', 'fraqu√≠ssimo', 'dc', 'terminando', 'salva', 'ne3djimi', 'httpstcoqdcgrtdyfp', 'atriz', 'conpletamente', 'cruise', 'outro', 'httpstcod1npcwm4uf', 'imaginar', 'nunca', 'vou', 'diverti', 'snyder', '3patrulha', 'httpstcobc50acicdw', 'at√©', 'lilmarks1', 'gosta', 's√©rie', 'daddy', 'amando', 'focar', '9chal', 'mera', 'vezes', 'dublado', 'maravilhosa', 'walfritsch', 'triste', 'am√©rica', 'faltando', 'passada', 'parece', '‚Äúvil√µes', 'come√ßa', 'legado', 'harley', 'semana', 'neg√≥cios', 'decepcionado', 'pueblakleydson', 'santosfc', 'ajuda', 'qual', '1shangchi', 'maneiro', 'httpstcoyjrzd4g2vk', 'adolescente', 'esquadr√£o', 'cor', 'rosto_chorando_de_rir', 'pirocona', 'gilson', 'violentcrims', 'horas', 'feedbacks', 'me', 'j√°', 'now', 'quinn', 'furiosos', 'nerdboomer', 'importa', 'john', 'p√°ginas', 'p', 'homi', 'filme', 'katana', '√©', 's√≥', 'criado', 'coelho', 'cenas', 'data', 'pregui√ßa', 't√£o', 'off', 'sweet', 'particularmente', 'aguardo', 'frleticya', '1boss', 'mt', 'pare√ßa', 'pedro', '4', 'tu', 'repente', 'robbie', 'davidbqwie', 'poderosa', 'personagens', 'frango', 'aves', 'sei', 'com√©dia', '6esquadr√£o', 'ent√£o', 'mem√≥ria', 'seguidos', 'gaga', 'estreia', 'diz', 'sangrento', 'vermelho', 'primeiro‚Ä¶', 'desde', 'eu', 'podia', 'interessar', 'shark', 'clime', 'fofo', 'top10movies', 'rosto_chorando_aos_berros', 'margot', 'httpstcohjlqydin9q', '140', 'odio', 'pare√ßo', 'ganhamos', 'httpstcoaxxvi8yagu', 'canina', 'recentemente', 'inflitrado', \"hq's\", 'fant√°stico', 'cleo', 'quer', 'hein', 'descobri', 'arrecadou', 'horrivel', 'acabei', 'et', '2015', 'qualidade', 'inteiro', 'v4', 'chamo', 'interpretado', 'mts', 'suic√≠da', 'daniel', 'passado', 'super', 'prazeres', 'chato', 'savitargod333', 'problemas', 'gabopantaleao', 'favorito', 'httpstcozt5k6osvta', 'esse', \"6d'artac√£o\", 'luafengarie', 'irma', 've', '1¬∞', '√≥bvio', 'karol', 'surrou', 'dccomics', 'maravilhoso', 'aclamada', '7a', 'pior', 'meu', 'juliette', 'httpstcojwhnlt6fux', 'httpstcolgnjorkwxo', 'liarodriguex', 'action', 'httpstco5wv9gje0it', 'carai', 'finalmente', 'pqp', 'games', 'alta_tens√£o', 'free', 'porrada', '2', 'cheguei', 'esquadrao', 'suicida', '6', 'justi√ßa', 'hq', 'google', 'longe', 'm√™s', 'envelopes', 'ri', 'classificativa', 'gunn', 'trilha', 'duduzera25', 'infiltrado', 'piadas', 'rosto_sorridente_com_olhos_de_cora√ß√£o', 'celestvics', 'chocado', 'vale', 'helena', 'tik', 'jungle', 'pena', 'morre', 'peacemaker', '8mist√©rio', 'hbomaxbr', '2‚Ä¶‚Ä¶', 'colho', 'iriam', 'ei', 'ok', 'podre', 'aquaman', 'saindo', 'putt', 'pessoas', 'a', 'recentes', 'jonh', 'maratonar', 'httpstcosgvodey2pe', 'queria', 'absurdo', 'nordeste', 'gabrielnerdland', 'termos', 'horrores', 'dinheiro', '5free', '4patrulha', '25set', 'no√ß√£o', 'hora', 'rosto_expressando_desagrado', 'dublada', 'brabo', 'p√©', 'pensando', 'juliettefinfos', 'rec√©mchegados', 'at', '‚ö†', 'seguidas', 'promo√ß√£o', 'tal', 'caralhoo', 'coisa', 'odiooo', 'depois', 'httpstcofntxc7xjme', 'refer√™ncia', 'todomundoodeiaochris', 'henriquenarizz', 'assiste', 'das', 'kageycat', '√¥', 'enredo', 'algu√©m', '710', 'algumas', 'craig', 'tarde', 'criticar', 'amigo', 'sexta', 'portugu√™s', 'ruanfalco', 'produtora', 'furiosa', 'casa', '230', 'detetive_mulher_pele_morena', 'chorei', 'lix√£o', 'bizarro', 'essa', 'chorando', 'httpstcoe99lfeue8u', '60', 'mortes', 'gun', 'antes', 'fantasy', 'colocar', 'n√£o', 'hora‚Ä¶', 'aneis', 'infinita', 'dora', 'dubla', 'for√ßa', 'isso¬ø¬ø¬ø‚ÄΩ‚ÄΩ', 'gloriagroove', 'eternos', 'quadrinhos', '1', 'opini√µes', 'mas', 'saco', 'domingo', 'fiz', 'kkkk', 'ingresso', 'prr', 'pr√≥ximo', 'fica', 'pois', 'prettylittleliars', 'marvel', 'patrulha', 'apagando', 'mes', 'timeline', 'ilhadenot√≠cias', 'pega', 'dez', 'volumes', 'fodase', 'de', 'esquadraosuicida', 'coisas', 'episodio', 'palha', 'carinhosamente', 'marola', 'gra√ßa', 'membros', 'morrer', 'dei', 'rato', 'httpstcogemtnxbeka', 'bicho', 'selecionou', 'da', 'tiram', 'bi', 'a√ßo', 'dirigiu', 'fotos', 'aventureira', 'usou', 'vei', 'oxentepipoca', '2free', 'partes', 's√©rio', '500', 'dcfandome', 'emocionadaaaa', 'starro', '2tava', 'silencioso', 'restaurante', 'a√≠', '41m‚Ç¨', 'afirmou', 'setembro', 'm√£o_em_v_de_vit√≥ria_pele_clara', 'revendo', 'mano', 'hj', 'ver', '17478383', 'tristes', 'balangar', 'loco', 'heroi', 'httpstcox3iolxrmiw', 'aten√ß√£o', '98m‚Ç¨', 'desencontro', 'rapina', 'dois', 'mortais', 'inesperada', 'dia', 'thesuicidesquad', 'descobrir', 'v√©i', 'pprt', 'agitado', 'top', 'sinal', 'vente', 'preferia', 'simplesmente', 'qualquer', 'solo', 'minutos', 'expectativas', 'moral', 'n√©', 'ser', 'nova', 'to', 'queen', 'ficou', 'do', 'lago', 'programa', 'gabo', 'elemento', 'pediu', 'mymaddiecade', 'ilhasolteira', 'acaso', 'ultimato', 'dubl√™', 'deus', 'gnt', 'boas', 'rainhas', 'descendo', 'rosto_com_olho_piscando', 'alvo', 'superar', 'ihugkugisaki', 'tooth', 'erros', 'junto', 'ningu√©m', 'daniel123desa', 'merece', 'jonhcena', 'opini√£o', 'ratcatcher', 'mal', 'ele', 'ganhou', 'cartaz', 's√£o', 'gduvivier', 'ja', 'ultimamente', 'segundo', 'fato', 'aquela', 'tvcinema', 'achando', 'lan√ßados', 'estavam', 'gomez', 'arruinando', '10', 'tentando', 'muuuito', 'obra', 'mundialmente', 'entrega', 'gl√≥ria', 'and', 'httpstcosalo5vig3z', 'idiota', 'venom', 'meses', 'infestada', 'assistam', 'shang', 'dono', 'gore', 'bom', 'pata', 'gloria', 'mn', 'httpstcovnpavh1elh', 'cartoons', 'coringa', 'd√≥', 'seus', '√∫ltimos', 'capit√£o', 'aquele', 'matths_az', 'causa', 'trinca', 'news', 'r2206', 'falar', 'desperdi√ßaram', 'puto', 'assisti', 'rosto_implorando', 'piores', 'chefinho', 'assustade', 'rosto_de_cabe√ßa_para_baixo', 'film', 'daqui', 'gravaram', 'd√≠vida', '5candyman', 'terem', 'doin', 'üìΩ', 'worst', 'crime', 'kombat', 'rolando_no_ch√£o_de_rir', 'verde', '3', 'httpstcogjrnqdqctp', 'vc', 'cortar', 'in', 'olha', 'k', 'quem', 'wooyngoth', 'quanto', 'necessito', 'editado', 'tanto', 'fundo', 'chega', 'merda', 's√©tima', 'assisto', 'viu', 'uns', '35‚òÖ', 'unico', 'ratos', 'prestar', 'top10', 'httpstcotpjc90fxwq', 'kkkkkkkkkk', 'entregou', 'vil√£o', 'jack', 'claro', 'esperando', 'rosto_sorridente_com_√≥culos_escuros', 'polkadotman', 'tambem', 'sos', 'cient√≠fica', 'jogar', 'lan√ßou', 'dentro', 'mulher', 'grande', 'vida', 'ow', 'leticya', 'guy', 'wual', 'primeiros', 'ate', 'ap√≥s', 'remake', 'httpstcotbktnxjmdx', 'criminal', 'kkkkkkkkkkkkkkkk', 'terminei', 'ajudar', 'passarinhos', 'conseguir', 'primeira', 'rosto_nauseado', 'sexualidade', 'nem', 'eles', 'metade', '12', 'cr√≠tica', 'quando', 'ramo', 'posso', 'outra', 'm√£os_juntas_pele_clara', 'toda', 'ruins', 'Ô∏èspoiler', 'nome', 'idade', 'lugar', 'riot', 'unicamente', 'olhei', 'amanh√£', 'representatividade', 'que', 'rosto_fazendo_sinal_de_sil√™ncio', 'apelidei', 'tenho', 'superou', 'camiseta', 'lembro', 'final', 'dceu', 'littlecansada', 'httpstcoou0u5lra7w', 'vi√∫va', 'bem', 'ganha', 'pacificador', 'cueca', 'alice', 'foram', 'do+', 'fiquei', 'sai', 'kashruno', 'palavreado', 'explica√ß√£o', 'porra', 'otimo', 'eh', 'te', '7candyman', 'desse', 'parecendo', 'podcast', 'arlequinha', 'falou', 'nham', '80', 'vcs', 'v√°rios', 'ia', 'mtt', 'zeldinha', 'tubar√£o', 'resumo', 'conseguiria', 'ca√ßa', 'lady', 'nossaaaa', 'gigante', '8', 'david', 'rey', '2021', 'dnv', 'meme', 'comento', 'cara', 'httpstcounpzka0x8t', 'colapso', 'voz', 'evagelion', 'wanda', 'provavelmente', 'q', 'chave', 'supera', 'sempre', 'kct', 'noia', 'deizao', 'favorita', 'primeiro', 'l√°', 'rei', 'perdi', 'interagir', 'estou', 'brasileira', 'solto', 'httpstcolk5ktnokoa', '9nem', '90', 'esmurra', 'comum', 'arqueiro', 'assistir', 'p√°', 'umas', 'ksksksksksks', 'dias', 'del√≠rio', '1pedrokkkkk', 'numa', 'salas', 'brutal', 'ah', 'dif√≠cil', 'morte'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# probabilidade de ser relevante:\r\n",
    "pr = len(listadr)/len(lista_total1)\r\n",
    "# probabilidade de ser irrelevante:\r\n",
    "pi = len(listadir)/len(lista_total1)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "print(pr+pi)\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def laplace(frequencia_absoluta,palavras_pertecentes_a_classe):\r\n",
    "    \"\"\"\"\"\r\n",
    "    Retorna a probabilidade independente da palavra analisada pertencer ou n√£o\r\n",
    "    a determinada classe\r\n",
    "    \"\"\"\r\n",
    "    return (frequencia_absoluta+1)/((palavras_pertecentes_a_classe)+len(Banco_de_dados))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "def prob_R(text):\r\n",
    "    Prob1 = 1\r\n",
    "    for i in text.split():\r\n",
    "        if i in fpdr:\r\n",
    "\r\n",
    "            Alaplace_relevante = laplace(fpdr[i], pr)\r\n",
    "        \r\n",
    "            Prob1 *= Alaplace_relevante\r\n",
    "        else:\r\n",
    "            Alaplace_relevante = laplace(0, pr)\r\n",
    "        \r\n",
    "            Prob1 *= Alaplace_relevante\r\n",
    "    return Prob1\r\n",
    "\r\n",
    "print(prob_R('assistir esse esquadr√£o suicida 2 de marola'))\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8.31732905864517e-23\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "def prob_I(text):\r\n",
    "    Prob2 = 1\r\n",
    "    for i in text.split():\r\n",
    "        if i in fpdir:\r\n",
    "\r\n",
    "            Alaplace_irrelevante = laplace(fpdir[i], pi)\r\n",
    "        \r\n",
    "            Prob2 *= Alaplace_irrelevante\r\n",
    "        else:\r\n",
    "            Alaplace_irrelevante = laplace(0, pi)\r\n",
    "        \r\n",
    "            Prob2 *= Alaplace_irrelevante\r\n",
    "    return Prob2\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "if prob_R('haterdogui') * pr >=prob_I('haterdogui')*pi:\r\n",
    "    print('MAROLAAAAAAA')\r\n",
    "else:\r\n",
    "    print(\"N√ÉOOO\")   \r\n",
    "\r\n",
    " "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "N√ÉOOO\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "frase = 'esquadrao suicida √© muito bom'\r\n",
    "n = 1\r\n",
    "for i in fpdir[frase.split()]:\r\n",
    "    n *= i \r\n",
    "Probpdir = n   \r\n",
    "print(Probpdir)\r\n",
    "\r\n",
    "n = 1\r\n",
    "for i in fpdr[frase.split()]:\r\n",
    "    n *= i \r\n",
    "Probpdr = n   \r\n",
    "print(Probpdr)\r\n",
    "\r\n",
    "n = 1\r\n",
    "for i in fptr[frase.split()]:\r\n",
    "    n *= i \r\n",
    "Probfptr = n   \r\n",
    "print(Probfptr)\r\n",
    "\r\n",
    "n = 1\r\n",
    "for i in fptir[frase.split()]:\r\n",
    "    n *= i\r\n",
    "Probfptir = n   \r\n",
    "print(Probfptir)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.3847596269482953e-12\n",
      "1.8288089274852862e-09\n",
      "2.159033147307012e-09\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "\"Passing list-likes to .loc or [] with any missing labels is no longer supported. The following labels were missing: Index(['bom'], dtype='object'). See https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\"",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-e8281b561402>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfptir\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfrase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mn\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mProbfptir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    904\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 906\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    907\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_with\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m         \u001b[1;31m# handle the dup indexing case GH#4246\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_values_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    877\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 879\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1097\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot index with multidimensional key\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m             \u001b[1;31m# nested tuple slicing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m         \u001b[1;31m# A collection of keys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m         \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m         return self.obj._reindex_with_indexers(\n\u001b[0;32m   1039\u001b[0m             \u001b[1;33m{\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1255\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0moption_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"display.max_seq_items\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"display.width\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1315\u001b[1;33m                     raise KeyError(\n\u001b[0m\u001b[0;32m   1316\u001b[0m                         \u001b[1;34m\"Passing list-likes to .loc or [] with any missing labels \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1317\u001b[0m                         \u001b[1;34m\"is no longer supported. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Passing list-likes to .loc or [] with any missing labels is no longer supported. The following labels were missing: Index(['bom'], dtype='object'). See https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\""
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "### Verificando a performance do Classificador\n",
    "\n",
    "Agora voc√™ deve testar o seu classificador com a base de Testes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "### Concluindo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "### Qualidade do Classificador a partir de novas separa√ß√µes dos tweets entre Treinamento e Teste\n",
    "\n",
    "Caso for fazer esse item do Projeto"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "## Aperfei√ßoamento:\n",
    "\n",
    "Trabalhos que conseguirem pelo menos conceito B v√£o evoluir em conceito dependendo da quantidade de itens avan√ßados:\n",
    "\n",
    "* IMPLEMENTOU outras limpezas e transforma√ß√µes que n√£o afetem a qualidade da informa√ß√£o contida nos tweets. Ex: stemming, lemmatization, stopwords\n",
    "* CORRIGIU separa√ß√£o de espa√ßos entre palavras e emojis ou entre emojis e emojis\n",
    "* CRIOU categorias intermedi√°rias de relev√¢ncia baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante. Pelo menos quatro categorias, com adi√ß√£o de mais tweets na base, conforme enunciado. (OBRIGAT√ìRIO PARA TRIOS, sem contar como item avan√ßado)\n",
    "* EXPLICOU porqu√™ n√£o pode usar o pr√≥prio classificador para gerar mais amostras de treinamento\n",
    "* PROP√îS diferentes cen√°rios para Na√Øve Bayes fora do contexto do projeto\n",
    "* SUGERIU e EXPLICOU melhorias reais com indica√ß√µes concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "* FEZ o item 6. Qualidade do Classificador a partir de novas separa√ß√µes dos tweets entre Treinamento e Teste descrito no enunciado do projeto (OBRIGAT√ìRIO para conceitos A ou A+)"
   ],
   "metadata": {},
   "attachments": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "## Refer√™ncias"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "8003219c8c57211ee3be347d121ba14ebad7276cdae3d94be72d9e4e17f9edd5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}